{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aUB4xrFdLkr8"
      },
      "outputs": [],
      "source": [
        "restart = True\n",
        "epoch_to_pickup = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import contextlib\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import gc  # Import the garbage collector module\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUgiww4oQ75T",
        "outputId": "5dab77bb-a78e-490b-abbd-0aafe8fcf0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "irakMtGnaImf"
      },
      "outputs": [],
      "source": [
        "path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nDl6_okDOUyY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path = '/content/drive/My Drive/M6_Fall2023e/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4r-dKnSRKz"
      },
      "source": [
        "## Functions for downloading text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xzLUaBa2Xmnb"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.replace(\"Project Gutenberg\", \"\")\n",
        "    text = text.replace(\"Gutenberg\", \"\")\n",
        "\n",
        "    # Remove carriage returns\n",
        "    text = text.replace(\"\\r\", \"\")\n",
        "\n",
        "    # fix quotes\n",
        "    text = text.replace(\"“\", \"\\\"\")\n",
        "    text = text.replace(\"”\", \"\\\"\")\n",
        "\n",
        "    # Replace any capital letter at the start of a word with ^ followed by the lowercase letter\n",
        "    text = re.sub(r\"(?<![a-zA-Z])([A-Z])\", lambda match: f\"^{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Replace all other capital letters with lowercase\n",
        "    text = re.sub(r\"([A-Z])\", lambda match: f\"{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Remove duplicate whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\"\\t+\", \"\\t\", text)\n",
        "\n",
        "    # Replace whitespace characters with special words\n",
        "    text = re.sub(r\"(\\t)\", r\" zztabzz \", text)\n",
        "    text = re.sub(r\"(\\n)\", r\" zznewlinezz \", text)\n",
        "    text = re.sub(r\"(\\s)\", r\" zzspacezz \", text)\n",
        "\n",
        "    # Split before and after punctuation\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, f\" {punctuation} \")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nFKehxVF9AxD"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(text):\n",
        "\n",
        "    # Replace special words with whitespace characters\n",
        "    text = text.replace(\"zztabzz\", \"\\t\")\n",
        "    text = text.replace(\"zznewlinezz\", \"\\n\")\n",
        "    text = text.replace(\"zzspacezz\", \" \")\n",
        "\n",
        "    # Remake capital letters at beginning of words\n",
        "    text = re.sub(r\"\\^([a-z])\", lambda match: f\"{match.group(1).upper()}\", text)\n",
        "\n",
        "    text = text.replace(\"^\", \"\")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Rtd9QyvUWqzi"
      },
      "outputs": [],
      "source": [
        "# def getMyText():\n",
        "#   path_to_file = tf.keras.utils.get_file('austen.txt', 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt')\n",
        "\n",
        "#   text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#   # path_to_file = tf.keras.utils.get_file('903-0.txt', 'https://www.gutenberg.org/files/903/903-0.txt')\n",
        "#   # author_text += open(path_to_file, 'rb').read().decode(encoding='utf-8')[2999:-19194]\n",
        "#   # tf.io.gfile.remove(path_to_file)\n",
        "\n",
        "#   return preprocess_text(text)\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def getMyText():\n",
        "    file_name = 'Sonnets.txt'\n",
        "    file_url = 'Sonnets.txt'\n",
        "    local_dir = 'saved_files'  # Directory to save the file\n",
        "    local_path = os.path.join(local_dir, file_name)\n",
        "\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        if not os.path.exists(local_dir):\n",
        "            os.makedirs(local_dir)\n",
        "\n",
        "        # Check if the file exists locally\n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"File '{file_name}' found locally. Using it.\")\n",
        "        else:\n",
        "            print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
        "            # Download the file\n",
        "            downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
        "\n",
        "            # Save the downloaded file to the designated local directory\n",
        "            with open(downloaded_path, 'rb') as source_file:\n",
        "                with open(local_path, 'wb') as dest_file:\n",
        "                    dest_file.write(source_file.read())\n",
        "\n",
        "        # Read the file's contents\n",
        "        with open(local_path, 'rb') as file:\n",
        "            text = file.read().decode(encoding='utf-8')\n",
        "\n",
        "        return preprocess_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getMyText()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "rofy7hJ1iHVm",
        "outputId": "e5f5a27d-ed59-4445-8d68-92b11127a2bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'Sonnets.txt' found locally. Using it.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' -  -  -  zzspacezz  #  #  #  zzspacezz  *  *  ^ venus zzspacezz and zzspacezz  ^ adonis *  *  zzspacezz  ^ even zzspacezz as zzspacezz the zzspacezz sun zzspacezz with zzspacezz purple - colour’d zzspacezz face zzspacezz  ^ had zzspacezz ta’en zzspacezz his zzspacezz last zzspacezz leave zzspacezz of zzspacezz the zzspacezz weeping zzspacezz morn ,  zzspacezz  ^ rose - cheek’d zzspacezz  ^ adonis zzspacezz hied zzspacezz him zzspacezz to zzspacezz the zzspacezz chase ;  zzspacezz  ^ hunting zzspacezz he zzspacezz lov’d ,  zzspacezz but zzspacezz love zzspacezz he zzspacezz laugh’d zzspacezz to zzspacezz scorn ;  zzspacezz  ^ sick - thoughted zzspacezz  ^ venus zzspacezz makes zzspacezz amain zzspacezz unto zzspacezz him ,  zzspacezz  ^ and zzspacezz like zzspacezz a zzspacezz bold - fac’d zzspacezz suitor zzspacezz ‘gins zzspacezz to zzspacezz woo zzspacezz him .  zzspacezz  \"  ^ thrice zzspacezz fairer zzspacezz than zzspacezz myself ,  \"  zzspacezz thus zzspacezz she zzspacezz began ,  zzspacezz  \"  ^ the zzspacezz field’s zzspacezz chief zzspacezz flower ,  zzspacezz sweet zzspacezz above zzspacezz compare ,  zzspacezz  ^ stain zzspacezz to zzspacezz all zzspacezz nymphs ,  zzspacezz more zzspacezz lovely zzspacezz than zzspacezz a zzspacezz man ,  zzspacezz  ^ more zzspacezz white zzspacezz and zzspacezz red zzspacezz than zzspacezz doves zzspacezz or zzspacezz roses zzspacezz are :  zzspacezz  ^ nature zzspacezz that zzspacezz made zzspacezz thee ,  zzspacezz with zzspacezz herself zzspacezz at zzspacezz strife ,  zzspacezz  ^ saith zzspacezz that zzspacezz the zzspacezz world zzspacezz hath zzspacezz ending zzspacezz with zzspacezz thy zzspacezz life .  zzspacezz  ^ vouchsafe ,  zzspacezz thou zzspacezz wonder ,  zzspacezz to zzspacezz alight zzspacezz thy zzspacezz steed ,  zzspacezz  ^ and zzspacezz rein zzspacezz his zzspacezz proud zzspacezz head zzspacezz to zzspacezz the zzspacezz saddle - bow ;  zzspacezz  ^ if zzspacezz thou zzspacezz wilt zzspacezz deign zzspacezz this zzspacezz favour ,  zzspacezz for zzspacezz thy zzspacezz meed zzspacezz  ^ a zzspacezz thousand zzspacezz honey zzspacezz secrets zzspacezz shalt zzspacezz thou zzspacezz know :  zzspacezz  ^ here zzspacezz come zzspacezz and zzspacezz sit ,  zzspacezz where zzspacezz never zzspacezz serpent zzspacezz hisses ,  zzspacezz  ^ and zzspacezz being zzspacezz set ,  zzspacezz  ^ i’ll zzspacezz smother zzspacezz thee zzspacezz with zzspacezz kisses .  \"  zzspacezz  -  -  -  zzspacezz  #  #  #  zzspacezz  *  *  ^ the zzspacezz  ^ rape zzspacezz of zzspacezz  ^ lucrece *  *  zzspacezz  ^ from zzspacezz the zzspacezz besieg’d zzspacezz  ^ ardea zzspacezz all zzspacezz in zzspacezz post ,  zzspacezz  ^ borne zzspacezz by zzspacezz the zzspacezz trustless zzspacezz wings zzspacezz of zzspacezz false zzspacezz desire ,  zzspacezz  ^ lust - breathing zzspacezz  ^ tarquin zzspacezz leaves zzspacezz the zzspacezz  ^ roman zzspacezz host ,  zzspacezz  ^ and zzspacezz to zzspacezz  ^ collatium zzspacezz bears zzspacezz the zzspacezz lightless zzspacezz fire ,  zzspacezz  ^ which ,  zzspacezz in zzspacezz pale zzspacezz embers zzspacezz hid ,  zzspacezz lurks zzspacezz to zzspacezz aspire zzspacezz  ^ and zzspacezz girdle zzspacezz with zzspacezz embracing zzspacezz flames zzspacezz the zzspacezz waist zzspacezz  ^ of zzspacezz  ^ collatine’s zzspacezz fair zzspacezz love ,  zzspacezz  ^ lucrece zzspacezz the zzspacezz chaste .  zzspacezz  ^ haply zzspacezz that zzspacezz name zzspacezz of zzspacezz ‘chaste’ zzspacezz unhapp’ly zzspacezz set zzspacezz  ^ this zzspacezz bateless zzspacezz edge zzspacezz on zzspacezz his zzspacezz keen zzspacezz appetite ;  zzspacezz  ^ when zzspacezz  ^ collatine zzspacezz unwisely zzspacezz did zzspacezz not zzspacezz let zzspacezz  ^ to zzspacezz praise zzspacezz the zzspacezz clear zzspacezz unmatched zzspacezz red zzspacezz and zzspacezz white zzspacezz  ^ which zzspacezz triumph’d zzspacezz in zzspacezz that zzspacezz sky zzspacezz of zzspacezz his zzspacezz delight ,  zzspacezz  ^ where zzspacezz mortal zzspacezz stars ,  zzspacezz as zzspacezz bright zzspacezz as zzspacezz heaven’s zzspacezz beauties ,  zzspacezz  ^ with zzspacezz pure zzspacezz aspects zzspacezz did zzspacezz him zzspacezz peculiar zzspacezz duties .  zzspacezz  -  -  -  zzspacezz  #  #  #  zzspacezz  *  *  ^ the zzspacezz  ^ passionate zzspacezz  ^ pilgrim *  *  zzspacezz  ^ when zzspacezz my zzspacezz love zzspacezz swears zzspacezz that zzspacezz she zzspacezz is zzspacezz made zzspacezz of zzspacezz truth ,  zzspacezz  ^ i zzspacezz do zzspacezz believe zzspacezz her ,  zzspacezz though zzspacezz  ^ i zzspacezz know zzspacezz she zzspacezz lies ,  zzspacezz  ^ that zzspacezz she zzspacezz might zzspacezz think zzspacezz me zzspacezz some zzspacezz untutored zzspacezz youth ,  zzspacezz  ^ unskilful zzspacezz in zzspacezz the zzspacezz world’s zzspacezz false zzspacezz forgeries .  zzspacezz  ^ thus zzspacezz vainly zzspacezz thinking zzspacezz that zzspacezz she zzspacezz thinks zzspacezz me zzspacezz young ,  zzspacezz  ^ although zzspacezz she zzspacezz knows zzspacezz my zzspacezz days zzspacezz are zzspacezz past zzspacezz the zzspacezz best ,  zzspacezz  ^ simply zzspacezz  ^ i zzspacezz credit zzspacezz her zzspacezz false - speaking zzspacezz tongue :  zzspacezz  ^ on zzspacezz both zzspacezz sides zzspacezz thus zzspacezz is zzspacezz simple zzspacezz truth zzspacezz suppress’d .  zzspacezz  ^ but zzspacezz wherefore zzspacezz says zzspacezz she zzspacezz not zzspacezz she zzspacezz is zzspacezz unjust ?  zzspacezz  ^ and zzspacezz wherefore zzspacezz say zzspacezz not zzspacezz  ^ i zzspacezz that zzspacezz  ^ i zzspacezz am zzspacezz old ?  zzspacezz  ^ o ,  zzspacezz love’s zzspacezz best zzspacezz habit zzspacezz is zzspacezz in zzspacezz seeming zzspacezz trust ,  zzspacezz  ^ and zzspacezz age zzspacezz in zzspacezz love zzspacezz loves zzspacezz not zzspacezz to zzspacezz have zzspacezz years zzspacezz told .  zzspacezz  ^ therefore zzspacezz  ^ i zzspacezz lie zzspacezz with zzspacezz her zzspacezz and zzspacezz she zzspacezz with zzspacezz me ,  zzspacezz  ^ and zzspacezz in zzspacezz our zzspacezz faults zzspacezz by zzspacezz lies zzspacezz we zzspacezz flatter’d zzspacezz be .  zzspacezz  -  -  -  zzspacezz  ^ sonnet zzspacezz 1 zzspacezz  ^ from zzspacezz fairest zzspacezz creatures zzspacezz we zzspacezz desire zzspacezz increase ,  zzspacezz  ^ that zzspacezz thereby zzspacezz beauty’s zzspacezz rose zzspacezz might zzspacezz never zzspacezz die ,  zzspacezz  ^ but zzspacezz as zzspacezz the zzspacezz riper zzspacezz should zzspacezz by zzspacezz time zzspacezz decease ,  zzspacezz  ^ his zzspacezz tender zzspacezz heir zzspacezz might zzspacezz bear zzspacezz his zzspacezz memory :  zzspacezz  ^ but zzspacezz thou zzspacezz contracted zzspacezz to zzspacezz thine zzspacezz own zzspacezz bright zzspacezz eyes ,  zzspacezz  ^ feed’st zzspacezz thy zzspacezz light’s zzspacezz flame zzspacezz with zzspacezz self - substantial zzspacezz fuel ,  zzspacezz  ^ making zzspacezz a zzspacezz famine zzspacezz where zzspacezz abundance zzspacezz lies ,  zzspacezz  ^ thyself zzspacezz thy zzspacezz foe ,  zzspacezz to zzspacezz thy zzspacezz sweet zzspacezz self zzspacezz too zzspacezz cruel .  zzspacezz  ^ thou zzspacezz that zzspacezz art zzspacezz now zzspacezz the zzspacezz world’s zzspacezz fresh zzspacezz ornament zzspacezz  ^ and zzspacezz only zzspacezz herald zzspacezz to zzspacezz the zzspacezz gaudy zzspacezz spring ,  zzspacezz  ^ within zzspacezz thine zzspacezz own zzspacezz bud zzspacezz buriest zzspacezz thy zzspacezz content ,  zzspacezz  ^ and ,  zzspacezz tender zzspacezz churl ,  zzspacezz mak’st zzspacezz waste zzspacezz in zzspacezz niggarding .  zzspacezz  ^ pity zzspacezz the zzspacezz world ,  zzspacezz or zzspacezz else zzspacezz this zzspacezz glutton zzspacezz be ,  zzspacezz  ^ to zzspacezz eat zzspacezz the zzspacezz world’s zzspacezz due ,  zzspacezz by zzspacezz the zzspacezz grave zzspacezz and zzspacezz thee .  zzspacezz  ^ sonnet zzspacezz 18 zzspacezz  ^ shall zzspacezz  ^ i zzspacezz compare zzspacezz thee zzspacezz to zzspacezz a zzspacezz summer’s zzspacezz day ?  zzspacezz  ^ thou zzspacezz art zzspacezz more zzspacezz lovely zzspacezz and zzspacezz more zzspacezz temperate :  zzspacezz  ^ rough zzspacezz winds zzspacezz do zzspacezz shake zzspacezz the zzspacezz darling zzspacezz buds zzspacezz of zzspacezz  ^ may ,  zzspacezz  ^ and zzspacezz summer’s zzspacezz lease zzspacezz hath zzspacezz all zzspacezz too zzspacezz short zzspacezz a zzspacezz date :  zzspacezz  ^ sometime zzspacezz too zzspacezz hot zzspacezz the zzspacezz eye zzspacezz of zzspacezz heaven zzspacezz shines ,  zzspacezz  ^ and zzspacezz often zzspacezz is zzspacezz his zzspacezz gold zzspacezz complexion zzspacezz dimm’d ;  zzspacezz  ^ and zzspacezz every zzspacezz fair zzspacezz from zzspacezz fair zzspacezz sometime zzspacezz declines ,  zzspacezz  ^ by zzspacezz chance zzspacezz or zzspacezz nature’s zzspacezz changing zzspacezz course zzspacezz untrimm’d ;  zzspacezz  ^ but zzspacezz thy zzspacezz eternal zzspacezz summer zzspacezz shall zzspacezz not zzspacezz fade zzspacezz  ^ nor zzspacezz lose zzspacezz possession zzspacezz of zzspacezz that zzspacezz fair zzspacezz thou zzspacezz owest ;  zzspacezz  ^ nor zzspacezz shall zzspacezz death zzspacezz brag zzspacezz thou zzspacezz wanderest zzspacezz in zzspacezz his zzspacezz shade ,  zzspacezz  ^ when zzspacezz in zzspacezz eternal zzspacezz lines zzspacezz to zzspacezz time zzspacezz thou zzspacezz growest :  zzspacezz  ^ so zzspacezz long zzspacezz as zzspacezz men zzspacezz can zzspacezz breathe zzspacezz or zzspacezz eyes zzspacezz can zzspacezz see ,  zzspacezz  ^ so zzspacezz long zzspacezz lives zzspacezz this ,  zzspacezz and zzspacezz this zzspacezz gives zzspacezz life zzspacezz to zzspacezz thee .  zzspacezz  ^ sonnet zzspacezz 29 zzspacezz  ^ when ,  zzspacezz in zzspacezz disgrace zzspacezz with zzspacezz fortune zzspacezz and zzspacezz men’s zzspacezz eyes ,  zzspacezz  ^ i zzspacezz all zzspacezz alone zzspacezz beweep zzspacezz my zzspacezz outcast zzspacezz state ,  zzspacezz  ^ and zzspacezz trouble zzspacezz deaf zzspacezz heaven zzspacezz with zzspacezz my zzspacezz bootless zzspacezz cries ,  zzspacezz  ^ and zzspacezz look zzspacezz upon zzspacezz myself ,  zzspacezz and zzspacezz curse zzspacezz my zzspacezz fate ,  zzspacezz  ^ wishing zzspacezz me zzspacezz like zzspacezz to zzspacezz one zzspacezz more zzspacezz rich zzspacezz in zzspacezz hope ,  zzspacezz  ^ featured zzspacezz like zzspacezz him ,  zzspacezz like zzspacezz him zzspacezz with zzspacezz friends zzspacezz possess’d ,  zzspacezz  ^ desiring zzspacezz this zzspacezz man’s zzspacezz art ,  zzspacezz and zzspacezz that zzspacezz man’s zzspacezz scope ,  zzspacezz  ^ with zzspacezz what zzspacezz  ^ i zzspacezz most zzspacezz enjoy zzspacezz contented zzspacezz least ;  zzspacezz  ^ yet zzspacezz in zzspacezz these zzspacezz thoughts zzspacezz myself zzspacezz almost zzspacezz despising ,  zzspacezz  ^ haply zzspacezz  ^ i zzspacezz think zzspacezz on zzspacezz thee ,  zzspacezz and zzspacezz then zzspacezz my zzspacezz state ,  zzspacezz  ^ like zzspacezz to zzspacezz the zzspacezz lark zzspacezz at zzspacezz break zzspacezz of zzspacezz day zzspacezz arising zzspacezz  ^ from zzspacezz sullen zzspacezz earth ,  zzspacezz sings zzspacezz hymns zzspacezz at zzspacezz heaven’s zzspacezz gate ;  zzspacezz  ^ for zzspacezz thy zzspacezz sweet zzspacezz love zzspacezz remember’d zzspacezz such zzspacezz wealth zzspacezz brings zzspacezz  ^ that zzspacezz then zzspacezz  ^ i zzspacezz scorn zzspacezz to zzspacezz change zzspacezz my zzspacezz state zzspacezz with zzspacezz kings .  zzspacezz  ^ sonnet zzspacezz 30 zzspacezz  ^ when zzspacezz to zzspacezz the zzspacezz sessions zzspacezz of zzspacezz sweet zzspacezz silent zzspacezz thought zzspacezz  ^ i zzspacezz summon zzspacezz up zzspacezz remembrance zzspacezz of zzspacezz things zzspacezz past ,  zzspacezz  ^ i zzspacezz sigh zzspacezz the zzspacezz lack zzspacezz of zzspacezz many zzspacezz a zzspacezz thing zzspacezz  ^ i zzspacezz sought ,  zzspacezz  ^ and zzspacezz with zzspacezz old zzspacezz woes zzspacezz new zzspacezz wail zzspacezz my zzspacezz dear zzspacezz time’s zzspacezz waste :  zzspacezz  ^ then zzspacezz can zzspacezz  ^ i zzspacezz drown zzspacezz an zzspacezz eye ,  zzspacezz unused zzspacezz to zzspacezz flow ,  zzspacezz  ^ for zzspacezz precious zzspacezz friends zzspacezz hid zzspacezz in zzspacezz death’s zzspacezz dateless zzspacezz night ,  zzspacezz  ^ and zzspacezz weep zzspacezz afresh zzspacezz love’s zzspacezz long - since - cancell’d zzspacezz woe ,  zzspacezz  ^ and zzspacezz moan zzspacezz the zzspacezz expense zzspacezz of zzspacezz many zzspacezz a zzspacezz vanish’d zzspacezz sight :  zzspacezz  ^ then zzspacezz can zzspacezz  ^ i zzspacezz grieve zzspacezz at zzspacezz grievances zzspacezz foregone ,  zzspacezz  ^ and zzspacezz heavily zzspacezz from zzspacezz woe zzspacezz to zzspacezz woe zzspacezz tell zzspacezz o’er zzspacezz  ^ the zzspacezz sad zzspacezz account zzspacezz of zzspacezz fore - bemoaned zzspacezz moan ,  zzspacezz  ^ which zzspacezz  ^ i zzspacezz new zzspacezz pay zzspacezz as zzspacezz if zzspacezz not zzspacezz paid zzspacezz before .  zzspacezz  ^ but zzspacezz if zzspacezz the zzspacezz while zzspacezz  ^ i zzspacezz think zzspacezz on zzspacezz thee ,  zzspacezz dear zzspacezz friend ,  zzspacezz  ^ all zzspacezz losses zzspacezz are zzspacezz restored ,  zzspacezz and zzspacezz sorrows zzspacezz end .  zzspacezz  ^ sonnet zzspacezz 33 zzspacezz  ^ full zzspacezz many zzspacezz a zzspacezz glorious zzspacezz morning zzspacezz have zzspacezz  ^ i zzspacezz seen zzspacezz  ^ flatter zzspacezz the zzspacezz mountain - tops zzspacezz with zzspacezz sovereign zzspacezz eye ,  zzspacezz  ^ kissing zzspacezz with zzspacezz golden zzspacezz face zzspacezz the zzspacezz meadows zzspacezz green ,  zzspacezz  ^ gilding zzspacezz pale zzspacezz streams zzspacezz with zzspacezz heavenly zzspacezz alchemy ;  zzspacezz  ^ anon zzspacezz permit zzspacezz the zzspacezz basest zzspacezz clouds zzspacezz to zzspacezz ride zzspacezz  ^ with zzspacezz ugly zzspacezz rack zzspacezz on zzspacezz his zzspacezz celestial zzspacezz face ,  zzspacezz  ^ and zzspacezz from zzspacezz the zzspacezz forlorn zzspacezz world zzspacezz his zzspacezz visage zzspacezz hide ,  zzspacezz  ^ stealing zzspacezz unseen zzspacezz to zzspacezz west zzspacezz with zzspacezz this zzspacezz disgrace :  zzspacezz  ^ even zzspacezz so zzspacezz my zzspacezz sun zzspacezz one zzspacezz early zzspacezz morn zzspacezz did zzspacezz shine ,  zzspacezz  ^ with zzspacezz all zzspacezz triumphant zzspacezz splendour zzspacezz on zzspacezz my zzspacezz brow ;  zzspacezz  ^ but ,  zzspacezz out ,  zzspacezz alack !  zzspacezz he zzspacezz was zzspacezz but zzspacezz one zzspacezz hour zzspacezz mine ;  zzspacezz  ^ the zzspacezz region zzspacezz cloud zzspacezz hath zzspacezz mask’d zzspacezz him zzspacezz from zzspacezz me zzspacezz now .  zzspacezz  ^ yet zzspacezz him zzspacezz for zzspacezz this zzspacezz my zzspacezz love zzspacezz no zzspacezz whit zzspacezz disdaineth ;  zzspacezz  ^ suns zzspacezz of zzspacezz the zzspacezz world zzspacezz may zzspacezz stain zzspacezz when zzspacezz heaven’s zzspacezz sun zzspacezz staineth .  zzspacezz  ^ sonnet zzspacezz 55 zzspacezz  ^ not zzspacezz marble ,  zzspacezz nor zzspacezz the zzspacezz gilded zzspacezz monuments zzspacezz  ^ of zzspacezz princes ,  zzspacezz shall zzspacezz outlive zzspacezz this zzspacezz powerful zzspacezz rhyme ;  zzspacezz  ^ but zzspacezz you zzspacezz shall zzspacezz shine zzspacezz more zzspacezz bright zzspacezz in zzspacezz these zzspacezz contents zzspacezz  ^ than zzspacezz unswept zzspacezz stone ,  zzspacezz besmear’d zzspacezz with zzspacezz sluttish zzspacezz time .  zzspacezz  ^ when zzspacezz wasteful zzspacezz war zzspacezz shall zzspacezz statues zzspacezz overturn ,  zzspacezz  ^ and zzspacezz broils zzspacezz root zzspacezz out zzspacezz the zzspacezz work zzspacezz of zzspacezz masonry ,  zzspacezz  ^ nor zzspacezz  ^ mars zzspacezz his zzspacezz sword ,  zzspacezz nor zzspacezz war’s zzspacezz quick zzspacezz fire zzspacezz shall zzspacezz burn zzspacezz  ^ the zzspacezz living zzspacezz record zzspacezz of zzspacezz your zzspacezz memory .  zzspacezz ‘ ^ gainst zzspacezz death ,  zzspacezz and zzspacezz all - oblivious zzspacezz enmity zzspacezz  ^ shall zzspacezz you zzspacezz pace zzspacezz forth ;  zzspacezz your zzspacezz praise zzspacezz shall zzspacezz still zzspacezz find zzspacezz room zzspacezz  ^ even zzspacezz in zzspacezz the zzspacezz eyes zzspacezz of zzspacezz all zzspacezz posterity zzspacezz  ^ that zzspacezz wear zzspacezz this zzspacezz world zzspacezz out zzspacezz to zzspacezz the zzspacezz ending zzspacezz doom .  zzspacezz  ^ so ,  zzspacezz till zzspacezz the zzspacezz judgment zzspacezz that zzspacezz yourself zzspacezz arise ,  zzspacezz  ^ you zzspacezz live zzspacezz in zzspacezz this ,  zzspacezz and zzspacezz dwell zzspacezz in zzspacezz lovers’ zzspacezz eyes .  zzspacezz  ^ sonnet zzspacezz 60 zzspacezz  ^ like zzspacezz as zzspacezz the zzspacezz waves zzspacezz make zzspacezz towards zzspacezz the zzspacezz pebbled zzspacezz shore ,  zzspacezz  ^ so zzspacezz do zzspacezz our zzspacezz minutes zzspacezz hasten zzspacezz to zzspacezz their zzspacezz end ;  zzspacezz  ^ each zzspacezz changing zzspacezz place zzspacezz with zzspacezz that zzspacezz which zzspacezz goes zzspacezz before ,  zzspacezz  ^ in zzspacezz sequent zzspacezz toil zzspacezz all zzspacezz forwards zzspacezz do zzspacezz contend .  zzspacezz  ^ nativity ,  zzspacezz once zzspacezz in zzspacezz the zzspacezz main zzspacezz of zzspacezz light ,  zzspacezz  ^ crawls zzspacezz to zzspacezz maturity ,  zzspacezz wherewith zzspacezz being zzspacezz crown’d ,  zzspacezz  ^ crooked zzspacezz eclipses zzspacezz ’gainst zzspacezz his zzspacezz glory zzspacezz fight ,  zzspacezz  ^ and zzspacezz  ^ time zzspacezz that zzspacezz gave zzspacezz doth zzspacezz now zzspacezz his zzspacezz gift zzspacezz confound .  zzspacezz  ^ time zzspacezz doth zzspacezz transfix zzspacezz the zzspacezz flourish zzspacezz set zzspacezz on zzspacezz youth zzspacezz  ^ and zzspacezz delves zzspacezz the zzspacezz parallels zzspacezz in zzspacezz beauty’s zzspacezz brow ,  zzspacezz  ^ feeds zzspacezz on zzspacezz the zzspacezz rarities zzspacezz of zzspacezz nature’s zzspacezz truth ,  zzspacezz  ^ and zzspacezz nothing zzspacezz stands zzspacezz but zzspacezz for zzspacezz his zzspacezz scythe zzspacezz to zzspacezz mow :  zzspacezz  ^ and zzspacezz yet zzspacezz to zzspacezz times zzspacezz in zzspacezz hope ,  zzspacezz my zzspacezz verse zzspacezz shall zzspacezz stand ,  zzspacezz  ^ praising zzspacezz thy zzspacezz worth ,  zzspacezz despite zzspacezz his zzspacezz cruel zzspacezz hand .  zzspacezz  ^ sonnet zzspacezz 73 zzspacezz  ^ that zzspacezz time zzspacezz of zzspacezz year zzspacezz thou zzspacezz mayst zzspacezz in zzspacezz me zzspacezz behold zzspacezz  ^ when zzspacezz yellow zzspacezz leaves ,  zzspacezz or zzspacezz none ,  zzspacezz or zzspacezz few ,  zzspacezz do zzspacezz hang zzspacezz  ^ upon zzspacezz those zzspacezz boughs zzspacezz which zzspacezz shake zzspacezz against zzspacezz the zzspacezz cold ,  zzspacezz  ^ bare zzspacezz ruin’d zzspacezz choirs ,  zzspacezz where zzspacezz late zzspacezz the zzspacezz sweet zzspacezz birds zzspacezz sang .  zzspacezz  ^ in zzspacezz me zzspacezz thou zzspacezz seest zzspacezz the zzspacezz twilight zzspacezz of zzspacezz such zzspacezz day zzspacezz  ^ as zzspacezz after zzspacezz sunset zzspacezz fadeth zzspacezz in zzspacezz the zzspacezz west ,  zzspacezz  ^ which zzspacezz by zzspacezz and zzspacezz by zzspacezz black zzspacezz night zzspacezz doth zzspacezz take zzspacezz away ,  zzspacezz  ^ death’s zzspacezz second zzspacezz self ,  zzspacezz that zzspacezz seals zzspacezz up zzspacezz all zzspacezz in zzspacezz rest .  zzspacezz  ^ in zzspacezz me zzspacezz thou zzspacezz seest zzspacezz the zzspacezz glowing zzspacezz of zzspacezz such zzspacezz fire zzspacezz  ^ that zzspacezz on zzspacezz the zzspacezz ashes zzspacezz of zzspacezz his zzspacezz youth zzspacezz doth zzspacezz lie ,  zzspacezz  ^ as zzspacezz the zzspacezz death - bed zzspacezz whereon zzspacezz it zzspacezz must zzspacezz expire ,  zzspacezz  ^ consumed zzspacezz with zzspacezz that zzspacezz which zzspacezz it zzspacezz was zzspacezz nourish’d zzspacezz by .  zzspacezz  ^ this zzspacezz thou zzspacezz perceivest ,  zzspacezz which zzspacezz makes zzspacezz thy zzspacezz love zzspacezz more zzspacezz strong ,  zzspacezz  ^ to zzspacezz love zzspacezz that zzspacezz well zzspacezz which zzspacezz thou zzspacezz must zzspacezz leave zzspacezz ere zzspacezz long .  zzspacezz  ^ sonnet zzspacezz 87 zzspacezz  ^ farewell !  zzspacezz thou zzspacezz art zzspacezz too zzspacezz dear zzspacezz for zzspacezz my zzspacezz possessing ,  zzspacezz  ^ and zzspacezz like zzspacezz enough zzspacezz thou zzspacezz know’st zzspacezz thy zzspacezz estimate :  zzspacezz  ^ the zzspacezz charter zzspacezz of zzspacezz thy zzspacezz worth zzspacezz gives zzspacezz thee zzspacezz releasing ;  zzspacezz  ^ my zzspacezz bonds zzspacezz in zzspacezz thee zzspacezz are zzspacezz all zzspacezz determinate .  zzspacezz  ^ for zzspacezz how zzspacezz do zzspacezz  ^ i zzspacezz hold zzspacezz thee zzspacezz but zzspacezz by zzspacezz thy zzspacezz granting ?  zzspacezz  ^ and zzspacezz for zzspacezz that zzspacezz riches zzspacezz where zzspacezz is zzspacezz my zzspacezz deserving ?  zzspacezz  ^ the zzspacezz cause zzspacezz of zzspacezz this zzspacezz fair zzspacezz gift zzspacezz in zzspacezz me zzspacezz is zzspacezz wanting ,  zzspacezz  ^ and zzspacezz so zzspacezz my zzspacezz patent zzspacezz back zzspacezz again zzspacezz is zzspacezz swerving .  zzspacezz  ^ thyself zzspacezz thou zzspacezz gavest ,  zzspacezz thy zzspacezz own zzspacezz worth zzspacezz then zzspacezz not zzspacezz knowing ,  zzspacezz  ^ or zzspacezz me ,  zzspacezz to zzspacezz whom zzspacezz thou zzspacezz gavest zzspacezz it ,  zzspacezz else zzspacezz mistaking ;  zzspacezz  ^ so zzspacezz thy zzspacezz great zzspacezz gift ,  zzspacezz upon zzspacezz misprision zzspacezz growing ,  zzspacezz  ^ comes zzspacezz home zzspacezz again ,  zzspacezz on zzspacezz better zzspacezz judgment zzspacezz making .  zzspacezz  ^ thus zzspacezz have zzspacezz  ^ i zzspacezz had zzspacezz thee ,  zzspacezz as zzspacezz a zzspacezz dream zzspacezz doth zzspacezz flatter ,  zzspacezz  ^ in zzspacezz sleep zzspacezz a zzspacezz king ,  zzspacezz but zzspacezz waking zzspacezz no zzspacezz such zzspacezz matter .  zzspacezz  ^ sonnet zzspacezz 116 zzspacezz  ^ let zzspacezz me zzspacezz not zzspacezz to zzspacezz the zzspacezz marriage zzspacezz of zzspacezz true zzspacezz minds zzspacezz  ^ admit zzspacezz impediments .  zzspacezz  ^ love zzspacezz is zzspacezz not zzspacezz love zzspacezz  ^ which zzspacezz alters zzspacezz when zzspacezz it zzspacezz alteration zzspacezz finds ,  zzspacezz  ^ or zzspacezz bends zzspacezz with zzspacezz the zzspacezz remover zzspacezz to zzspacezz remove :  zzspacezz  ^ o zzspacezz no !  zzspacezz it zzspacezz is zzspacezz an zzspacezz ever - fixed zzspacezz mark zzspacezz  ^ that zzspacezz looks zzspacezz on zzspacezz tempests ,  zzspacezz and zzspacezz is zzspacezz never zzspacezz shaken ;  zzspacezz  ^ it zzspacezz is zzspacezz the zzspacezz star zzspacezz to zzspacezz every zzspacezz wandering zzspacezz bark ,  zzspacezz  ^ whose zzspacezz worth’s zzspacezz unknown ,  zzspacezz although zzspacezz his zzspacezz height zzspacezz be zzspacezz taken .  zzspacezz  ^ love’s zzspacezz not zzspacezz  ^ time’s zzspacezz fool ,  zzspacezz though zzspacezz rosy zzspacezz lips zzspacezz and zzspacezz cheeks zzspacezz  ^ within zzspacezz his zzspacezz bending zzspacezz sickle’s zzspacezz compass zzspacezz come :  zzspacezz  ^ love zzspacezz alters zzspacezz not zzspacezz with zzspacezz his zzspacezz brief zzspacezz hours zzspacezz and zzspacezz weeks ,  zzspacezz  ^ but zzspacezz bears zzspacezz it zzspacezz out zzspacezz even zzspacezz to zzspacezz the zzspacezz edge zzspacezz of zzspacezz doom .  zzspacezz  ^ if zzspacezz this zzspacezz be zzspacezz error zzspacezz and zzspacezz upon zzspacezz me zzspacezz proved ,  zzspacezz  ^ i zzspacezz never zzspacezz writ ,  zzspacezz nor zzspacezz no zzspacezz man zzspacezz ever zzspacezz loved .  zzspacezz  ^ uh ,  zzspacezz uh zzspacezz  ^ uh ,  zzspacezz uh zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ wake zzspacezz up ,  zzspacezz  ^ mr .  zzspacezz  ^ west ,  zzspacezz  ^ mr .  zzspacezz  ^ west zzspacezz  ^ mr .  zzspacezz  ^ fresh ,  zzspacezz  ^ mr .  zzspacezz  ^ by zzspacezz  ^ himself ,  zzspacezz he \\' s zzspacezz so zzspacezz impressed zzspacezz  ^ i zzspacezz mean ,  zzspacezz dang ,  zzspacezz did zzspacezz you zzspacezz even zzspacezz see zzspacezz the zzspacezz test ?  zzspacezz  ^ you zzspacezz got zzspacezz  ^ d \\' s ,  zzspacezz man ,  zzspacezz  ^ d \\' s ,  zzspacezz  ^ rosie zzspacezz  ^ perez zzspacezz  ^ and zzspacezz yer zzspacezz barely zzspacezz passed zzspacezz any zzspacezz and zzspacezz every zzspacezz class zzspacezz  ^ lookin \\'  zzspacezz at zzspacezz every zzspacezz girl ,  zzspacezz cheated zzspacezz on zzspacezz every zzspacezz test zzspacezz  ^ i zzspacezz guess ,  zzspacezz this zzspacezz is zzspacezz my zzspacezz dissertation zzspacezz  ^ homie ,  zzspacezz this zzspacezz stuff zzspacezz is zzspacezz basic ,  zzspacezz welcome zzspacezz to zzspacezz  ^ graduation zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  ,  zzspacezz on zzspacezz this zzspacezz day zzspacezz we zzspacezz become zzspacezz legendary zzspacezz  ^ everythin \\'  zzspacezz we zzspacezz dreamed zzspacezz of zzspacezz  ^ i \\' m zzspacezz like zzspacezz the zzspacezz fly zzspacezz  ^ malcolm zzspacezz  ^ x ,  zzspacezz buy zzspacezz any zzspacezz jeans zzspacezz necessary zzspacezz  ^ detroit zzspacezz  ^ red zzspacezz cleaned zzspacezz up zzspacezz  ^ from zzspacezz the zzspacezz streets zzspacezz to zzspacezz the zzspacezz league ,  zzspacezz from zzspacezz an zzspacezz eighth zzspacezz to zzspacezz a zzspacezz key zzspacezz  ^ but zzspacezz you zzspacezz graduate zzspacezz when zzspacezz you zzspacezz make zzspacezz it zzspacezz up zzspacezz outta zzspacezz the zzspacezz streets zzspacezz  ^ from zzspacezz the zzspacezz moments zzspacezz of zzspacezz pain ,  zzspacezz look zzspacezz how zzspacezz far zzspacezz we zzspacezz done zzspacezz came zzspacezz  ^ haters zzspacezz sayin \\'  zzspacezz you zzspacezz changed ,  zzspacezz now zzspacezz you zzspacezz doin \\'  zzspacezz your zzspacezz thing zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz and— zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz and zzspacezz look zzspacezz at zzspacezz the zzspacezz valedictorian zzspacezz  ^ scared zzspacezz of zzspacezz the zzspacezz future zzspacezz while zzspacezz  ^ i zzspacezz hop zzspacezz in zzspacezz the zzspacezz  ^ delorean zzspacezz  ^ scared zzspacezz to zzspacezz face zzspacezz the zzspacezz world ,  zzspacezz complacent zzspacezz career zzspacezz student zzspacezz  ^ some zzspacezz people zzspacezz graduate ,  zzspacezz but zzspacezz be zzspacezz still zzspacezz stupid zzspacezz  ^ they zzspacezz tell zzspacezz you ,  zzspacezz  \"  ^ read zzspacezz this ,  zzspacezz eat zzspacezz this ,  zzspacezz don \\' t zzspacezz look zzspacezz around zzspacezz  ^ just zzspacezz peep zzspacezz this ,  zzspacezz preach zzspacezz this ,  zzspacezz teach zzspacezz us ,  zzspacezz  ^ jesus \"  zzspacezz  ^ okay ,  zzspacezz look zzspacezz up zzspacezz now ,  zzspacezz they zzspacezz done zzspacezz stole zzspacezz yo \\'  zzspacezz streetness zzspacezz  ^ after zzspacezz all zzspacezz of zzspacezz that ,  zzspacezz you zzspacezz received zzspacezz this zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ hustlers ,  zzspacezz that \\' s zzspacezz if zzspacezz you \\' re zzspacezz still zzspacezz livin \\'  ,  zzspacezz get zzspacezz on zzspacezz down zzspacezz  ^ every zzspacezz time zzspacezz that zzspacezz we zzspacezz hear zzspacezz them zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ hustlers ,  zzspacezz that \\' s zzspacezz if zzspacezz you \\' re zzspacezz still zzspacezz livin \\'  ,  zzspacezz get zzspacezz on zzspacezz down zzspacezz  ^ every zzspacezz time zzspacezz that zzspacezz we zzspacezz hear zzspacezz them zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ hustlers ,  zzspacezz that \\' s zzspacezz if zzspacezz you \\' re zzspacezz still zzspacezz livin \\'  ,  zzspacezz get zzspacezz on zzspacezz down zzspacezz  ^ every zzspacezz time zzspacezz that zzspacezz we zzspacezz hear zzspacezz them zzspacezz  ^ good zzspacezz mornin \\'  zzspacezz  ^ hustlers ,  zzspacezz that \\' s zzspacezz if zzspacezz you \\' re zzspacezz still zzspacezz livin \\'  ,  zzspacezz get zzspacezz on zzspacezz down zzspacezz  ^ get zzspacezz on zzspacezz down zzspacezz  ^ get— zzspacezz  ^ get zzspacezz on zzspacezz down zzspacezz  ^ get zzspacezz on zzspacezz down zzspacezz  ^ get— zzspacezz  ^ get zzspacezz on zzspacezz down zzspacezz  -  -  -  zzspacezz  ^ did zzspacezz you zzspacezz realize zzspacezz  ^ that zzspacezz you zzspacezz were zzspacezz a zzspacezz champion zzspacezz in zzspacezz their zzspacezz eyes ?  zzspacezz  ^ yes ,  zzspacezz  ^ i zzspacezz did zzspacezz  ^ so zzspacezz  ^ i zzspacezz packed zzspacezz it zzspacezz up zzspacezz and zzspacezz brought zzspacezz it zzspacezz back zzspacezz to zzspacezz the zzspacezz crib zzspacezz  ^ just zzspacezz a zzspacezz lil \\'  zzspacezz somethin \\'  ,  zzspacezz show zzspacezz you zzspacezz how zzspacezz we zzspacezz live zzspacezz  ^ everybody zzspacezz want zzspacezz it zzspacezz but zzspacezz it zzspacezz ain \\' t zzspacezz that zzspacezz serious zzspacezz  ^ mhm ,  zzspacezz that \\' s zzspacezz that zzspacezz stuff zzspacezz  ^ so zzspacezz if zzspacezz you zzspacezz gon \\'  zzspacezz do zzspacezz it ,  zzspacezz do zzspacezz it zzspacezz just zzspacezz like zzspacezz this zzspacezz  ^ did zzspacezz you zzspacezz realize zzspacezz  ^ that zzspacezz you zzspacezz were zzspacezz a zzspacezz champion zzspacezz in zzspacezz their zzspacezz eyes ?  zzspacezz  ^ you zzspacezz don \\' t zzspacezz see zzspacezz just zzspacezz how zzspacezz wild zzspacezz the zzspacezz crowd zzspacezz is ?  zzspacezz  ^ you zzspacezz don \\' t zzspacezz see zzspacezz just zzspacezz how zzspacezz fly zzspacezz my zzspacezz style zzspacezz is ?  zzspacezz  ^ i zzspacezz don \\' t zzspacezz see zzspacezz why zzspacezz  ^ i zzspacezz need zzspacezz a zzspacezz stylist zzspacezz  ^ when zzspacezz  ^ i zzspacezz shop zzspacezz so zzspacezz much zzspacezz  ^ i zzspacezz can zzspacezz speak zzspacezz  ^ italian zzspacezz  ^ i zzspacezz don \\' t zzspacezz know ,  zzspacezz  ^ i zzspacezz just zzspacezz wanted zzspacezz better zzspacezz for zzspacezz my zzspacezz kids zzspacezz  ^ and zzspacezz  ^ i zzspacezz ain \\' t zzspacezz sayin \\'  zzspacezz we zzspacezz was zzspacezz from zzspacezz the zzspacezz projects zzspacezz  ^ but zzspacezz every zzspacezz time zzspacezz  ^ i zzspacezz wanted zzspacezz layaway zzspacezz or zzspacezz a zzspacezz deposit zzspacezz  ^ my zzspacezz dad \\' d zzspacezz say ,  zzspacezz  \"  ^ when zzspacezz you zzspacezz see zzspacezz clothes ,  zzspacezz close zzspacezz your zzspacezz eyelids \"  zzspacezz  ^ we zzspacezz was zzspacezz sort zzspacezz of zzspacezz like zzspacezz  ^ will zzspacezz  ^ smith zzspacezz and zzspacezz his zzspacezz son zzspacezz  ^ in zzspacezz the zzspacezz movie ,  zzspacezz  ^ i zzspacezz ain \\' t zzspacezz talkin \\'  zzspacezz  \\' bout zzspacezz the zzspacezz rich zzspacezz ones zzspacezz  \\'  ^ cause zzspacezz every zzspacezz summer zzspacezz he \\' d zzspacezz get zzspacezz some zzspacezz  ^ brand zzspacezz new zzspacezz hare - brained zzspacezz scheme zzspacezz to zzspacezz get zzspacezz rich zzspacezz from zzspacezz  ^ and zzspacezz  ^ i zzspacezz don \\' t zzspacezz know zzspacezz what zzspacezz he zzspacezz did zzspacezz for zzspacezz dough zzspacezz  ^ but zzspacezz he \\' d zzspacezz send zzspacezz me zzspacezz back zzspacezz to zzspacezz school zzspacezz with zzspacezz a zzspacezz new zzspacezz wardrobe ,  zzspacezz but zzspacezz ayy zzspacezz  ^ did zzspacezz you zzspacezz realize zzspacezz  ^ ayy ,  zzspacezz ayy ,  zzspacezz ayy zzspacezz  ^ that zzspacezz you zzspacezz were zzspacezz a zzspacezz champion zzspacezz in zzspacezz their zzspacezz eyes ?  zzspacezz  ^ i zzspacezz think zzspacezz he zzspacezz did zzspacezz  ^ when zzspacezz he zzspacezz packed zzspacezz it zzspacezz up zzspacezz and zzspacezz brought zzspacezz it zzspacezz back zzspacezz to zzspacezz the zzspacezz crib zzspacezz  ^ just zzspacezz a zzspacezz lil \\'  zzspacezz somethin \\'  ,  zzspacezz show zzspacezz you zzspacezz how zzspacezz we zzspacezz live zzspacezz  ^ everything zzspacezz  ^ i zzspacezz wanted ,  zzspacezz man ,  zzspacezz it zzspacezz seem zzspacezz so zzspacezz serious zzspacezz  ^ mhm ,  zzspacezz that \\' s zzspacezz that zzspacezz stuff zzspacezz  ^ so zzspacezz if zzspacezz you zzspacezz gon \\'  zzspacezz do zzspacezz it ,  zzspacezz do zzspacezz it zzspacezz just zzspacezz like zzspacezz this zzspacezz  ^ did zzspacezz you zzspacezz realize zzspacezz  ^ that zzspacezz you zzspacezz were zzspacezz a zzspacezz champion zzspacezz in zzspacezz their zzspacezz eyes ?  zzspacezz  ^ when zzspacezz it zzspacezz feel zzspacezz like zzspacezz living \\' s zzspacezz harder zzspacezz than zzspacezz dyin \\'  zzspacezz  ^ for zzspacezz me ,  zzspacezz givin \\'  zzspacezz up \\' s zzspacezz way zzspacezz harder zzspacezz than zzspacezz tryin \\'  zzspacezz  ^ lauryn zzspacezz  ^ hill zzspacezz said zzspacezz her zzspacezz heart zzspacezz was zzspacezz in zzspacezz  ^ zion zzspacezz  ^ i zzspacezz wish zzspacezz her zzspacezz heart zzspacezz still zzspacezz was zzspacezz in zzspacezz rhymin \\'  zzspacezz  \\'  ^ cause zzspacezz who zzspacezz the zzspacezz kids zzspacezz gon \\'  zzspacezz listen zzspacezz to ?  zzspacezz  ^ huh ?  zzspacezz  ^ i zzspacezz guess zzspacezz me zzspacezz if zzspacezz it zzspacezz isn \\' t zzspacezz you zzspacezz  ^ last zzspacezz week ,  zzspacezz  ^ i zzspacezz paid zzspacezz a zzspacezz visit zzspacezz to zzspacezz the zzspacezz institute zzspacezz  ^ they zzspacezz got zzspacezz the zzspacezz dropout zzspacezz keepin \\'  zzspacezz kids zzspacezz in zzspacezz the zzspacezz school zzspacezz  ^ i zzspacezz guess zzspacezz  ^ i \\' ll zzspacezz cleaned zzspacezz up zzspacezz my zzspacezz act zzspacezz like zzspacezz  ^ prince \\' ll zzspacezz do zzspacezz  ^ if zzspacezz not zzspacezz for zzspacezz the zzspacezz pleasure ,  zzspacezz least zzspacezz for zzspacezz the zzspacezz principle zzspacezz  ^ they zzspacezz got zzspacezz the zzspacezz  ^ cd ,  zzspacezz then zzspacezz got zzspacezz to zzspacezz see zzspacezz me zzspacezz  ^ drop zzspacezz gems zzspacezz like zzspacezz  ^ i zzspacezz dropped zzspacezz out zzspacezz of zzspacezz  ^ pe zzspacezz  ^ they zzspacezz used zzspacezz to zzspacezz feel zzspacezz invisible zzspacezz  ^ now ,  zzspacezz they zzspacezz know zzspacezz they zzspacezz invincible zzspacezz  ^ did zzspacezz you zzspacezz realize zzspacezz  ^ ayy ,  zzspacezz ayy ,  zzspacezz ayy zzspacezz  ^ that zzspacezz you zzspacezz were zzspacezz a zzspacezz champion zzspacezz in zzspacezz their zzspacezz eyes ?  zzspacezz  ^ this zzspacezz is zzspacezz the zzspacezz story zzspacezz of zzspacezz a zzspacezz champion zzspacezz  ^ runners zzspacezz on zzspacezz their zzspacezz mark zzspacezz and zzspacezz they zzspacezz pop zzspacezz their zzspacezz guns zzspacezz  ^ stand zzspacezz up ,  zzspacezz stand zzspacezz up ,  zzspacezz here zzspacezz he zzspacezz comes zzspacezz  ^ tell zzspacezz me zzspacezz what zzspacezz it zzspacezz takes zzspacezz to zzspacezz be zzspacezz number zzspacezz one zzspacezz  ^ tell zzspacezz me zzspacezz what zzspacezz it zzspacezz takes zzspacezz to zzspacezz be zzspacezz number zzspacezz one zzspacezz  ^ this zzspacezz is zzspacezz the zzspacezz story zzspacezz of zzspacezz a zzspacezz champion zzspacezz  ^ runners zzspacezz on zzspacezz their zzspacezz mark zzspacezz and zzspacezz they zzspacezz pop zzspacezz their zzspacezz guns zzspacezz  ^ stand zzspacezz up ,  zzspacezz stand zzspacezz up ,  zzspacezz here zzspacezz he zzspacezz comes zzspacezz  ^ tell zzspacezz me zzspacezz what zzspacezz it zzspacezz takes zzspacezz to zzspacezz be zzspacezz number zzspacezz one zzspacezz  ^ tell zzspacezz me zzspacezz what zzspacezz it zzspacezz takes zzspacezz to zzspacezz be zzspacezz number zzspacezz one zzspacezz  ^ did zzspacezz you zzspacezz realize zzspacezz  ^ that zzspacezz you zzspacezz were zzspacezz a zzspacezz champion zzspacezz in zzspacezz their zzspacezz eyes ?  zzspacezz  ^ yes ,  zzspacezz  ^ i zzspacezz did zzspacezz  ^ so zzspacezz  ^ i zzspacezz packed zzspacezz it zzspacezz up zzspacezz and zzspacezz brought zzspacezz it zzspacezz back zzspacezz to zzspacezz the zzspacezz crib zzspacezz  ^ just zzspacezz a zzspacezz lil \\'  zzspacezz somethin \\'  ,  zzspacezz show zzspacezz you zzspacezz how zzspacezz we zzspacezz live zzspacezz  ^ everybody zzspacezz want zzspacezz it zzspacezz but zzspacezz it zzspacezz ain \\' t zzspacezz that zzspacezz serious zzspacezz  ^ mhm ,  zzspacezz that \\' s zzspacezz that zzspacezz stuff zzspacezz  ^ so zzspacezz if zzspacezz you zzspacezz gon \\'  zzspacezz do zzspacezz it ,  zzspacezz do zzspacezz it zzspacezz just zzspacezz like zzspacezz this zzspacezz  ^ like zzspacezz this zzspacezz  ^ did zzspacezz you zzspacezz realize zzspacezz  ^ that zzspacezz you zzspacezz were zzspacezz a zzspacezz champion zzspacezz in zzspacezz their zzspacezz eyes ?  zzspacezz  -  -  -  zzspacezz  ^ here zzspacezz is zzspacezz the zzspacezz version zzspacezz of zzspacezz the zzspacezz lyrics zzspacezz with zzspacezz explicit zzspacezz words zzspacezz removed :  zzspacezz  -  -  -  zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ( yeah )  zzspacezz  ^ wait zzspacezz  \\' til zzspacezz  ^ i zzspacezz get zzspacezz my zzspacezz money zzspacezz right zzspacezz  ^ i zzspacezz had zzspacezz a zzspacezz dream zzspacezz  ^ i zzspacezz could zzspacezz buy zzspacezz my zzspacezz way zzspacezz to zzspacezz heaven zzspacezz  ^ when zzspacezz  ^ i zzspacezz awoke ,  zzspacezz  ^ i zzspacezz spent zzspacezz that zzspacezz on zzspacezz a zzspacezz necklace zzspacezz  ^ i zzspacezz told zzspacezz  ^ god zzspacezz  ^ i \\' d zzspacezz be zzspacezz back zzspacezz in zzspacezz a zzspacezz second zzspacezz  ^ man ,  zzspacezz it \\' s zzspacezz so zzspacezz hard zzspacezz not zzspacezz to zzspacezz act zzspacezz reckless zzspacezz  ^ to zzspacezz whom zzspacezz much zzspacezz is zzspacezz given ,  zzspacezz much zzspacezz is zzspacezz tested zzspacezz  ^ get zzspacezz arrested ,  zzspacezz guess zzspacezz until zzspacezz he zzspacezz get zzspacezz the zzspacezz message zzspacezz  ^ i zzspacezz feel zzspacezz the zzspacezz pressure ,  zzspacezz under zzspacezz more zzspacezz scrutiny zzspacezz  ^ and zzspacezz what zzspacezz  ^ i zzspacezz do ?  zzspacezz  ^ act zzspacezz more zzspacezz stupidly zzspacezz  ^ bought zzspacezz more zzspacezz jewelry ,  zzspacezz more zzspacezz  ^ louis zzspacezz  ^ v zzspacezz  ^ my zzspacezz  ^ mama zzspacezz couldn \\' t zzspacezz get zzspacezz through zzspacezz to zzspacezz me zzspacezz  ^ the zzspacezz drama ,  zzspacezz people zzspacezz suing zzspacezz me zzspacezz  ^ i \\' m zzspacezz on zzspacezz  ^ tv zzspacezz talking zzspacezz like zzspacezz it \\' s zzspacezz just zzspacezz you zzspacezz and zzspacezz me zzspacezz  ^ i \\' m zzspacezz just zzspacezz saying zzspacezz how zzspacezz  ^ i zzspacezz feel zzspacezz man zzspacezz  ^ i zzspacezz ain \\' t zzspacezz one zzspacezz of zzspacezz the zzspacezz  ^ cosby \\' s ,  zzspacezz  ^ i zzspacezz ain \\' t zzspacezz go zzspacezz to zzspacezz  ^ hillman zzspacezz  ^ i zzspacezz guess zzspacezz the zzspacezz money zzspacezz should \\' ve zzspacezz changed zzspacezz him zzspacezz  ^ i zzspacezz guess zzspacezz  ^ i zzspacezz should \\' ve zzspacezz forgot zzspacezz where zzspacezz  ^ i zzspacezz came zzspacezz from zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la ,  zzspacezz la zzspacezz  ( ayy )  zzspacezz  ^ wait zzspacezz  \\' til zzspacezz  ^ i zzspacezz get zzspacezz my zzspacezz money zzspacezz right zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ( yeah )  zzspacezz  ^ then zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing ,  zzspacezz right ?  zzspacezz  ^ excuse zzspacezz me ,  zzspacezz was zzspacezz you zzspacezz saying zzspacezz something ?  zzspacezz  ^ uh - uh ,  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  (  ^ haha )  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  ^ uh - uh ,  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  ^ let zzspacezz up zzspacezz the zzspacezz suicide zzspacezz doors zzspacezz  ^ this zzspacezz is zzspacezz my zzspacezz life ,  zzspacezz homie ,  zzspacezz you zzspacezz decide zzspacezz yours zzspacezz  ^ i zzspacezz know zzspacezz that zzspacezz  ^ jesus zzspacezz died zzspacezz for zzspacezz us zzspacezz  ^ but zzspacezz  ^ i zzspacezz couldn \\' t zzspacezz tell zzspacezz ya zzspacezz who zzspacezz decide zzspacezz wars zzspacezz  ^ so zzspacezz  ^ i zzspacezz parallel zzspacezz double zzspacezz parked zzspacezz that zzspacezz mother zzspacezz sideways zzspacezz  ^ old zzspacezz folks zzspacezz talking zzspacezz bout zzspacezz back zzspacezz in zzspacezz my zzspacezz day zzspacezz  ^ but zzspacezz homie zzspacezz this zzspacezz is zzspacezz my zzspacezz day zzspacezz  ^ class zzspacezz started zzspacezz two zzspacezz hours zzspacezz ago ,  zzspacezz oh ,  zzspacezz am zzspacezz  ^ i zzspacezz late ?  zzspacezz  ^ no ,  zzspacezz  ^ i zzspacezz already zzspacezz graduated zzspacezz  ^ and zzspacezz you zzspacezz can zzspacezz live zzspacezz through zzspacezz anything zzspacezz if zzspacezz  ^ magic zzspacezz made zzspacezz it zzspacezz  ^ they zzspacezz say zzspacezz  ^ i zzspacezz talk zzspacezz with zzspacezz so zzspacezz much zzspacezz emphasis zzspacezz  ^ ooh ,  zzspacezz they zzspacezz so zzspacezz sensitive zzspacezz  ^ don \\' t zzspacezz ever zzspacezz fix zzspacezz your zzspacezz lips zzspacezz like zzspacezz collagen zzspacezz  ^ and zzspacezz say zzspacezz something zzspacezz when zzspacezz you zzspacezz gon \\'  zzspacezz end zzspacezz up zzspacezz apologing zzspacezz  ^ let zzspacezz me zzspacezz know zzspacezz if zzspacezz it \\' s zzspacezz a zzspacezz problem zzspacezz then zzspacezz  ^ aight zzspacezz man ,  zzspacezz holla zzspacezz then zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ( ayy )  zzspacezz  ^ wait zzspacezz  \\' til zzspacezz  ^ i zzspacezz get zzspacezz my zzspacezz money zzspacezz right zzspacezz  ( yeah )  zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ( yeah )  zzspacezz  ^ then zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing ,  zzspacezz right ?  zzspacezz  ^ excuse zzspacezz me ,  zzspacezz was zzspacezz you zzspacezz saying zzspacezz something ?  zzspacezz  ^ uh - uh ,  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  (  ^ haha )  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  ^ uh - uh ,  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  ^ let zzspacezz the zzspacezz champagne zzspacezz splash zzspacezz  ^ let zzspacezz that zzspacezz man zzspacezz get zzspacezz cash zzspacezz  ^ let zzspacezz that zzspacezz man zzspacezz get zzspacezz past zzspacezz  ^ he zzspacezz don \\' t zzspacezz even zzspacezz stop zzspacezz to zzspacezz get zzspacezz gas zzspacezz  ^ if zzspacezz he zzspacezz can zzspacezz move zzspacezz through zzspacezz the zzspacezz rumors zzspacezz  ^ he zzspacezz can zzspacezz drive zzspacezz off zzspacezz of zzspacezz fumes zzspacezz  \\' cause zzspacezz  ^ how zzspacezz he zzspacezz move zzspacezz in zzspacezz a zzspacezz room zzspacezz full zzspacezz of zzspacezz no \\' s ?  zzspacezz  ^ how zzspacezz he zzspacezz stay zzspacezz faithful zzspacezz in zzspacezz a zzspacezz room zzspacezz full zzspacezz of .  .  .  zzspacezz  ^ must zzspacezz be zzspacezz the zzspacezz pharaohs ,  zzspacezz he zzspacezz in zzspacezz tune zzspacezz with zzspacezz his zzspacezz soul zzspacezz  ^ so zzspacezz when zzspacezz he zzspacezz buried zzspacezz in zzspacezz a zzspacezz tomb zzspacezz full zzspacezz of zzspacezz gold zzspacezz  ^ treasure ,  zzspacezz what \\' s zzspacezz your zzspacezz pleasure ?  zzspacezz  ^ life zzspacezz is zzspacezz a ,  zzspacezz uh ,  zzspacezz depending zzspacezz how zzspacezz you zzspacezz dress zzspacezz her zzspacezz  ^ so zzspacezz if zzspacezz the zzspacezz  ^ devil zzspacezz wear zzspacezz  ^ prada ,  zzspacezz  ^ adam zzspacezz  ^ eve zzspacezz wear zzspacezz nada zzspacezz  ^ i \\' m zzspacezz in zzspacezz between zzspacezz but zzspacezz way zzspacezz more zzspacezz fresher zzspacezz  ^ with zzspacezz way zzspacezz less zzspacezz effort zzspacezz  \\'  ^ cause zzspacezz when zzspacezz you zzspacezz try zzspacezz hard zzspacezz that \\' s zzspacezz when zzspacezz you zzspacezz die zzspacezz hard zzspacezz  ^ your zzspacezz homies zzspacezz looking zzspacezz like zzspacezz why zzspacezz  ^ god zzspacezz  ^ when zzspacezz they zzspacezz reminisce zzspacezz over zzspacezz you ,  zzspacezz my zzspacezz  ^ god zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ^ wait zzspacezz  \\' til zzspacezz  ^ i zzspacezz get zzspacezz my zzspacezz money zzspacezz right zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ^ then zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing ,  zzspacezz right ?  zzspacezz  ^ excuse zzspacezz me ,  zzspacezz was zzspacezz you zzspacezz saying zzspacezz something ?  zzspacezz  ^ uh - uh ,  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  (  ^ haha )  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  ^ uh - uh ,  zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing zzspacezz  ( yeah )  zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ^ wait zzspacezz  \\' til zzspacezz  ^ i zzspacezz get zzspacezz my zzspacezz money zzspacezz right zzspacezz  ^ la ,  zzspacezz la ,  zzspacezz la zzspacezz la zzspacezz  ^ then zzspacezz you zzspacezz can \\' t zzspacezz tell zzspacezz me zzspacezz nothing ,  zzspacezz right ?  zzspacezz  ^ ayy zzspacezz  ^ yeah zzspacezz  ^ hahaha zzspacezz  ^ yeah zzspacezz  ( yeah )  zzspacezz  ^ i \\' m zzspacezz serious ,  zzspacezz  ^ i zzspacezz got zzspacezz money zzspacezz  -  -  -  zzspacezz  ^ yeah zzspacezz  ^ and zzspacezz you zzspacezz say zzspacezz  ^ chi zzspacezz  ^ city zzspacezz  ^ chi zzspacezz  ^ city ,  zzspacezz  ^ chi zzspacezz  ^ city zzspacezz  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  (  ^ yeah )  zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  \\'  ^ cause zzspacezz  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again ,  zzspacezz  \\' min \\'  zzspacezz home zzspacezz again zzspacezz  ^ i zzspacezz met zzspacezz this zzspacezz girl zzspacezz when zzspacezz  ^ i zzspacezz was zzspacezz three zzspacezz years zzspacezz old zzspacezz  ^ and zzspacezz what zzspacezz  ^ i zzspacezz loved zzspacezz most ,  zzspacezz she zzspacezz had zzspacezz so zzspacezz much zzspacezz soul zzspacezz  ^ she zzspacezz said ,  zzspacezz excuse zzspacezz me ,  zzspacezz little zzspacezz homie ,  zzspacezz  ^ i zzspacezz know zzspacezz you zzspacezz don \\' t zzspacezz know zzspacezz me zzspacezz  ^ but ,  zzspacezz my zzspacezz name zzspacezz is zzspacezz  ^ wendy zzspacezz and zzspacezz  ^ i zzspacezz like zzspacezz to zzspacezz blow zzspacezz trees zzspacezz  ^ and zzspacezz from zzspacezz that zzspacezz point zzspacezz  ^ i zzspacezz never zzspacezz blow zzspacezz her zzspacezz off zzspacezz  ^ niggas zzspacezz come zzspacezz from zzspacezz outta zzspacezz town ,  zzspacezz  ^ i zzspacezz like zzspacezz to zzspacezz show zzspacezz her zzspacezz off zzspacezz  ^ they zzspacezz like zzspacezz to zzspacezz act zzspacezz tough ,  zzspacezz she zzspacezz like zzspacezz to zzspacezz tow zzspacezz  \\' em zzspacezz off zzspacezz  ^ and zzspacezz make zzspacezz  \\' em zzspacezz straighten zzspacezz up zzspacezz their zzspacezz act zzspacezz  \\' cause zzspacezz she zzspacezz know zzspacezz they zzspacezz soft zzspacezz  ^ and zzspacezz when zzspacezz  ^ i zzspacezz grew zzspacezz up ,  zzspacezz she zzspacezz showed zzspacezz me zzspacezz how zzspacezz to zzspacezz go zzspacezz downtown zzspacezz  ^ in zzspacezz the zzspacezz night zzspacezz time zzspacezz her zzspacezz face zzspacezz lit zzspacezz up ,  zzspacezz so zzspacezz astoundin \\'  zzspacezz  ^ and zzspacezz  ^ i zzspacezz told zzspacezz her zzspacezz in zzspacezz my zzspacezz heart zzspacezz is zzspacezz where zzspacezz she zzspacezz always zzspacezz be zzspacezz  ^ she zzspacezz never zzspacezz mess zzspacezz with zzspacezz entertainers zzspacezz  \\' cause zzspacezz they zzspacezz always zzspacezz leave zzspacezz  ^ she zzspacezz said zzspacezz it zzspacezz felt zzspacezz like zzspacezz they zzspacezz walked zzspacezz and zzspacezz drove zzspacezz on zzspacezz me zzspacezz  ^ knew zzspacezz  ^ i zzspacezz was zzspacezz gang zzspacezz affiliated ,  zzspacezz got zzspacezz on zzspacezz  ^ tv zzspacezz and zzspacezz told zzspacezz on zzspacezz me zzspacezz  ^ i zzspacezz guess zzspacezz it \\' s zzspacezz why zzspacezz last zzspacezz winter zzspacezz she zzspacezz got zzspacezz so zzspacezz cold zzspacezz on zzspacezz me zzspacezz  ^ she zzspacezz said ,  zzspacezz  \\'  ^ ye zzspacezz keep zzspacezz makin \\'  zzspacezz that zzspacezz  (  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again )  zzspacezz  ^ keep zzspacezz makin \\'  zzspacezz that zzspacezz platinum zzspacezz and zzspacezz gold zzspacezz for zzspacezz me zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  \\'  ^ cause zzspacezz  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again ,  zzspacezz  \\' min \\'  zzspacezz home zzspacezz again zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  ^ oh ,  zzspacezz now zzspacezz  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again zzspacezz  ^ maybe zzspacezz we zzspacezz could zzspacezz start zzspacezz again zzspacezz  ^ but zzspacezz if zzspacezz you zzspacezz really zzspacezz cared zzspacezz for zzspacezz her zzspacezz  ^ then zzspacezz you zzspacezz wouldn \\' t \\' ve zzspacezz never zzspacezz hit zzspacezz the zzspacezz airport zzspacezz  ^ to zzspacezz follow zzspacezz your zzspacezz dream ,  zzspacezz sometimes zzspacezz  ^ i zzspacezz still zzspacezz talk zzspacezz to zzspacezz her zzspacezz  ^ but zzspacezz when zzspacezz  ^ i zzspacezz talk zzspacezz to zzspacezz her ,  zzspacezz it zzspacezz always zzspacezz seems zzspacezz like zzspacezz she zzspacezz talkin \\'  zzspacezz about zzspacezz me zzspacezz  ^ she zzspacezz said ,  zzspacezz you zzspacezz left zzspacezz your zzspacezz kids zzspacezz and zzspacezz they zzspacezz just zzspacezz like zzspacezz you zzspacezz  ^ they zzspacezz wanna zzspacezz rap zzspacezz and zzspacezz make zzspacezz soul zzspacezz beats zzspacezz just zzspacezz like zzspacezz you zzspacezz  ^ but zzspacezz they zzspacezz just zzspacezz not zzspacezz you zzspacezz and zzspacezz  ^ i zzspacezz just zzspacezz got zzspacezz through zzspacezz  ^ talkin \\'  zzspacezz  \\' bout zzspacezz what zzspacezz niggas zzspacezz tryin \\'  zzspacezz to zzspacezz do zzspacezz just zzspacezz not zzspacezz new zzspacezz  ^ now zzspacezz everybody zzspacezz got zzspacezz the zzspacezz game zzspacezz figured zzspacezz out zzspacezz all zzspacezz wrong zzspacezz  ^ i zzspacezz guess zzspacezz you zzspacezz never zzspacezz know zzspacezz what zzspacezz you zzspacezz got zzspacezz  \\' til zzspacezz it \\' s zzspacezz gone zzspacezz  ^ i zzspacezz guess zzspacezz this zzspacezz is zzspacezz why zzspacezz  ^ i \\' m zzspacezz here zzspacezz and zzspacezz  ^ i zzspacezz can \\' t zzspacezz come zzspacezz back zzspacezz home zzspacezz  ^ and zzspacezz guess zzspacezz when zzspacezz  ^ i zzspacezz heard zzspacezz that ?  zzspacezz  ^ when zzspacezz  ^ i zzspacezz was zzspacezz back zzspacezz home zzspacezz  ^ every zzspacezz interview zzspacezz  ^ i \\' m zzspacezz representin \\'  zzspacezz you ,  zzspacezz makin \\'  zzspacezz you zzspacezz proud zzspacezz  ^ reach zzspacezz for zzspacezz the zzspacezz stars zzspacezz so zzspacezz if zzspacezz you zzspacezz fall zzspacezz you zzspacezz land zzspacezz on zzspacezz a zzspacezz cloud zzspacezz  ^ jump zzspacezz in zzspacezz the zzspacezz crowd ,  zzspacezz spark zzspacezz your zzspacezz lighters ,  zzspacezz wave zzspacezz  \\' em zzspacezz around zzspacezz  ^ and zzspacezz if zzspacezz you zzspacezz don \\' t zzspacezz know zzspacezz by zzspacezz now zzspacezz  (  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again )  zzspacezz  ^ i \\' m zzspacezz talkin \\'  zzspacezz about zzspacezz  ^ chi zzspacezz  ^ town zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  ^ do zzspacezz you zzspacezz think zzspacezz about zzspacezz me zzspacezz now zzspacezz and zzspacezz then ?  zzspacezz  \\'  ^ cause zzspacezz  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again ,  zzspacezz  \\' min \\'  zzspacezz home zzspacezz again zzspacezz  ^ baby ,  zzspacezz do zzspacezz you zzspacezz remember zzspacezz when zzspacezz fireworks zzspacezz at zzspacezz  ^ lake zzspacezz  ^ michigan ?  zzspacezz  ^ oh ,  zzspacezz now zzspacezz  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again ,  zzspacezz  \\' min \\'  zzspacezz home zzspacezz again zzspacezz  ^ baby ,  zzspacezz do zzspacezz you zzspacezz remember zzspacezz when zzspacezz fireworks zzspacezz at zzspacezz  ^ lake zzspacezz  ^ michigan ?  zzspacezz  ^ oh ,  zzspacezz now zzspacezz  ^ i \\' m zzspacezz comin \\'  zzspacezz home zzspacezz again zzspacezz  ^ maybe zzspacezz we zzspacezz could zzspacezz start zzspacezz again zzspacezz  ^ loy - oy - al ,  zzspacezz loy - oy - al zzspacezz  ^ comin \\'  zzspacezz home zzspacezz again zzspacezz  ^ loy - oy - al ,  zzspacezz loy - oy - al zzspacezz  ^ comin \\'  zzspacezz home zzspacezz again zzspacezz  ^ maybe zzspacezz we zzspacezz could zzspacezz start zzspacezz again zzspacezz '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gsCd-ihOU02C"
      },
      "outputs": [],
      "source": [
        "def getRandomText(numbooks = 1, verbose=False):\n",
        "  download_log = io.StringIO()\n",
        "  text_random = ''\n",
        "  for b in range(numbooks):\n",
        "    foundbook = False\n",
        "    while(foundbook == False):\n",
        "      booknum = random.randint(100,60000)\n",
        "      if verbose:\n",
        "        print('Trying Book #: ',booknum)\n",
        "      if random.random() > 0.5:\n",
        "        url = 'https://www.gutenberg.org/files/' + str(booknum) + '/' + str(booknum) + '-0.txt'\n",
        "        filename_temp = str(booknum) + '-0.txt'\n",
        "      else:\n",
        "        url = 'https://www.gutenberg.org/cache/epub/' + str(booknum) + '/pg' + str(booknum) + '.txt'\n",
        "        filename_temp = 'pg' + str(booknum) + '.txt'\n",
        "      if verbose:\n",
        "        print('Trying: ', url)\n",
        "      try:\n",
        "        if verbose:\n",
        "          path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
        "        else:\n",
        "          with contextlib.redirect_stdout(download_log):\n",
        "            path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
        "        temptext = open(path_to_file_temp, 'rb').read().decode(encoding='utf-8')\n",
        "        tf.io.gfile.remove(path_to_file_temp)\n",
        "        if (temptext.find('Language: English') >= 0):\n",
        "          offset = random.randint(-20,20)\n",
        "          header = 2000\n",
        "          total_length = 200000\n",
        "          chopoffend = 10000\n",
        "          if len(temptext) > (header+total_length+offset+chopoffend):\n",
        "            foundbook = True\n",
        "            text_random += temptext[header+offset:header+total_length+offset]\n",
        "            #print(\"Yes: \" + str(booknum))\n",
        "            if verbose:\n",
        "              print('New size of dataset: ', len(text_random))\n",
        "          elif len(temptext) > (header+12000):\n",
        "            foundbook = True\n",
        "            text_random += temptext[header:-chopoffend]\n",
        "            #print(\"Yes (smaller): \" + str(booknum))\n",
        "            if verbose:\n",
        "              print('New size of dataset: ', len(text_random))\n",
        "          else:\n",
        "            if verbose:\n",
        "              print('Not long enough. Trying again...')\n",
        "            #print(\"No: \" + str(booknum) + \" too short\")\n",
        "        else:\n",
        "          if verbose:\n",
        "            print('Not English. Trying again...')\n",
        "          #print(\"No: \" + str(booknum) + \" not English\")\n",
        "        del temptext\n",
        "      except:\n",
        "        if verbose:\n",
        "          print('Not valid file. Trying again...')\n",
        "        #print(\"No: \" + str(booknum) + \" not valid\")\n",
        "        foundbook = False\n",
        "    if verbose:\n",
        "      print(\"Found \" + str(b+1) + \" books so far...\")\n",
        "  del download_log\n",
        "  #text_random = \"\".join(c for c in text_random if c in vocab)\n",
        "  #all_ids_random = ids_from_chars(tf.strings.unicode_split(text_random, 'UTF-8'))\n",
        "  #ids_dataset_random = tf.data.Dataset.from_tensor_slices(all_ids_random)\n",
        "  #sequences_random = ids_dataset_random.batch(seq_length+1, drop_remainder=True)\n",
        "  #dataset_random = sequences_random.map(split_input_target)\n",
        "  #dataset_random = (dataset_random.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  #return dataset_random\n",
        "  return preprocess_text(text_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjEF0LKxhljS",
        "outputId": "60db283c-0b29-414f-8bd7-107c6699d17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'Sonnets.txt' found locally. Using it.\n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  vocab_text = getMyText()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpFvtyF_g3jY"
      },
      "source": [
        "Make vocabulary (Adapted from TensorFlow word embedding tutorial)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F8E6Q6dkMEpd"
      },
      "outputs": [],
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 1112\n",
        "sequence_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AWXUqLQ6g3KB"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  # Use the text vectorization layer to normalize, split, and map strings to\n",
        "  # integers. Note that the layer uses the custom standardization defined above.\n",
        "  # Set maximum_sequence length as all samples are not of the same length.\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize='lower',\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zJfr5w1bTWiJ"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "  vectorize_layer.adapt([vocab_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PmaoiyvF1Ilm"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocabulary = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ULNtM_8nYn"
      },
      "source": [
        "Save Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "G1hjxv447INt"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  with open(path + \"vocabulary.txt\", \"w\") as file:\n",
        "    for word in vocabulary:\n",
        "        file.write(word + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7qn5MjC8p0_"
      },
      "source": [
        "Load Saved Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TLbSoqUP8Pxu"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  with open(path + \"vocabulary.txt\", \"r\") as file:\n",
        "      vocabulary = [word.strip() for word in file.readlines()]\n",
        "      vocabulary = vocabulary\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      vocabulary=vocabulary,\n",
        "      standardize='lower',\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FidGlurb1iD3",
        "outputId": "f3e1acbe-7f57-420d-b9e5-5626783866c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'zzspacezz', '^', ',', \"'\", 'i', 'the', 'you', 'and', 'to', 'that', 'me', '-', 'in', 'of', '.', 'it', '?', 'a']\n",
            "['ago', 'age', 'against', 'afresh', 'affiliated', 'admit', 'adam', 'account', 'abundance', 'above', '87', '73', '60', '55', '33', '30', '29', '18', '116', '1']\n"
          ]
        }
      ],
      "source": [
        "print(vocabulary[:20])\n",
        "print(vocabulary[-20:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LovypAGk91Yp"
      },
      "source": [
        "Turn text into a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Mnp0huUX93Wi"
      },
      "outputs": [],
      "source": [
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_ids = sequence[:-1]\n",
        "    target_ids = sequence[1:]\n",
        "    return input_ids, target_ids\n",
        "\n",
        "# This function will create the dataset\n",
        "def text_to_dataset(text):\n",
        "  all_ids = vectorize_layer(text)\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "  del all_ids\n",
        "  sequences = ids_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "  del ids_dataset\n",
        "\n",
        "  # Call the function for every sequence in our list to create a new dataset\n",
        "  # of input->target pairs\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  del sequences\n",
        "\n",
        "  # shuffle\n",
        "\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afRybxef_QHi"
      },
      "source": [
        "Test on vocab text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0tBa6ttN_Ufz"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = text_to_dataset(vocab_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vq191mRgWv2w"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  text = ''.join([vocabulary[index] for index in ids])\n",
        "  return postprocess_text(text)\n",
        "\n",
        "vocabulary_adjusted = vocabulary\n",
        "vocabulary_adjusted[0] = '[UNK]'\n",
        "vocabulary_adjusted[1] = ''\n",
        "\n",
        "words_from_ids = tf.keras.layers.StringLookup(vocabulary=vocabulary_adjusted, invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDqaTHXFAEBD",
        "outputId": "c89e0e1d-7977-4a52-a6a8-e875c386d476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            "tf.Tensor(\n",
            "[  13   13   13    2   99   99   99    2   74   74    3  260    2    9\n",
            "    2    3  385   74   74    2    3  118    2   91    2    7    2  190\n",
            "    2   23    2  641   13  995    2  134    2    3  169    2  509    2\n",
            "   38    2  219    2  218    2   15    2    7    2  425    2  307    4\n",
            "    2    3  290   13 1007    2    3  385    2  812    2   96    2   10\n",
            "    2    7    2 1011   47    2    3  797    2   46    2  743    4    2\n",
            "   29    2   78    2   46    2  768    2   10    2  285   47    2    3\n",
            "  576   13  497    2    3  260    2  314    2 1082    2  460    2   96\n",
            "    4    2    3    9    2   27    2   19    2 1043   13  907    2  521\n",
            "    2  387], shape=(128,), dtype=int64)\n",
            "--- ### **Venus and Adonis** Even as the sun with purple-colour’d face Had ta’en his last leave of the weeping morn, Rose-cheek’d Adonis hied him to the chase; Hunting he lov’d, but love he laugh’d to scorn; Sick-thoughted Venus makes amain unto him, And like a bold-fac’d suitor ‘gins\n",
            "tf.Tensor(\n",
            "[b'-' b'-' b'-' b'zzspacezz' b'#' b'#' b'#' b'zzspacezz' b'*' b'*' b'^'\n",
            " b'venus' b'zzspacezz' b'and' b'zzspacezz' b'^' b'adonis' b'*' b'*'\n",
            " b'zzspacezz' b'^' b'even' b'zzspacezz' b'as' b'zzspacezz' b'the'\n",
            " b'zzspacezz' b'sun' b'zzspacezz' b'with' b'zzspacezz' b'purple' b'-'\n",
            " b'colour\\xe2\\x80\\x99d' b'zzspacezz' b'face' b'zzspacezz' b'^' b'had'\n",
            " b'zzspacezz' b'ta\\xe2\\x80\\x99en' b'zzspacezz' b'his' b'zzspacezz' b'last'\n",
            " b'zzspacezz' b'leave' b'zzspacezz' b'of' b'zzspacezz' b'the' b'zzspacezz'\n",
            " b'weeping' b'zzspacezz' b'morn' b',' b'zzspacezz' b'^' b'rose' b'-'\n",
            " b'cheek\\xe2\\x80\\x99d' b'zzspacezz' b'^' b'adonis' b'zzspacezz' b'hied'\n",
            " b'zzspacezz' b'him' b'zzspacezz' b'to' b'zzspacezz' b'the' b'zzspacezz'\n",
            " b'chase' b';' b'zzspacezz' b'^' b'hunting' b'zzspacezz' b'he'\n",
            " b'zzspacezz' b'lov\\xe2\\x80\\x99d' b',' b'zzspacezz' b'but' b'zzspacezz'\n",
            " b'love' b'zzspacezz' b'he' b'zzspacezz' b'laugh\\xe2\\x80\\x99d'\n",
            " b'zzspacezz' b'to' b'zzspacezz' b'scorn' b';' b'zzspacezz' b'^' b'sick'\n",
            " b'-' b'thoughted' b'zzspacezz' b'^' b'venus' b'zzspacezz' b'makes'\n",
            " b'zzspacezz' b'amain' b'zzspacezz' b'unto' b'zzspacezz' b'him' b','\n",
            " b'zzspacezz' b'^' b'and' b'zzspacezz' b'like' b'zzspacezz' b'a'\n",
            " b'zzspacezz' b'bold' b'-' b'fac\\xe2\\x80\\x99d' b'zzspacezz' b'suitor'\n",
            " b'zzspacezz' b'\\xe2\\x80\\x98gins'], shape=(128,), dtype=string)\n",
            "Target: \n",
            "tf.Tensor(\n",
            "[  13   13    2   99   99   99    2   74   74    3  260    2    9    2\n",
            "    3  385   74   74    2    3  118    2   91    2    7    2  190    2\n",
            "   23    2  641   13  995    2  134    2    3  169    2  509    2   38\n",
            "    2  219    2  218    2   15    2    7    2  425    2  307    4    2\n",
            "    3  290   13 1007    2    3  385    2  812    2   96    2   10    2\n",
            "    7    2 1011   47    2    3  797    2   46    2  743    4    2   29\n",
            "    2   78    2   46    2  768    2   10    2  285   47    2    3  576\n",
            "   13  497    2    3  260    2  314    2 1082    2  460    2   96    4\n",
            "    2    3    9    2   27    2   19    2 1043   13  907    2  521    2\n",
            "  387    2], shape=(128,), dtype=int64)\n",
            "-- ### **Venus and Adonis** Even as the sun with purple-colour’d face Had ta’en his last leave of the weeping morn, Rose-cheek’d Adonis hied him to the chase; Hunting he lov’d, but love he laugh’d to scorn; Sick-thoughted Venus makes amain unto him, And like a bold-fac’d suitor ‘gins \n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  for input_example, target_example in vocab_ds.take(1):\n",
        "    print(\"Input: \")\n",
        "    print(input_example)\n",
        "    print(text_from_ids(input_example))\n",
        "    print(words_from_ids(input_example))\n",
        "    print(\"Target: \")\n",
        "    print(target_example)\n",
        "    print(text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Rp402vgrS54t"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "def setup_dataset(dataset):\n",
        "  dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0LdoMfT7T8WN"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = setup_dataset(vocab_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQ-KjEeZMzd"
      },
      "source": [
        "## III. Build the model\n",
        "\n",
        "Next, we'll build our model. Up until this point, you've been using the Keras symbolic, or imperative API for creating your models. Doing something like:\n",
        "\n",
        "    model = tf.keras.models.Sequentla()\n",
        "    model.add(tf.keras.layers.Dense(80, activation='relu))\n",
        "    etc...\n",
        "\n",
        "However, tensorflow has another way to build models called the Functional API, which gives us a lot more control over what happens inside the model. You can read more about [the differences and when to use each here](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
        "\n",
        "We'll use the functional API for our RNN in this example. This will involve defining our model as a custom subclass of `tf.keras.Model`.\n",
        "\n",
        "If you're not familiar with classes in python, you might want to review [this quick tutorial](https://www.w3schools.com/python/python_classes.asp), as well as [this one on class inheritance](https://www.w3schools.com/python/python_inheritance.asp).\n",
        "\n",
        "Using a functional model is important for our situation because we're not just training it to predict a single character for a single sequence, but as we make predictions with it, we need it to remember those predictions as use that memory as it makes new predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Fj4uh9y-Y9mx"
      },
      "outputs": [],
      "source": [
        "# Create our custom model. Given a sequence of characters, this\n",
        "# model's job is to predict what character should come next.\n",
        "class AustenTextModel(tf.keras.Model):\n",
        "\n",
        "  # This is our class constructor method, it will be executed when\n",
        "  # we first create an instance of the class\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # Our model will have three layers:\n",
        "\n",
        "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
        "    #    a vector of values suitable for a neural network\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
        "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
        "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
        "    #    then consider trying out LSTM instead (or in addition to!)\n",
        "    #self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm3 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    #self.lstm4 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    self.hidden1 = tf.keras.layers.Dense(embedding_dim*64, activation='relu')\n",
        "    self.hidden2 = tf.keras.layers.Dense(embedding_dim*16, activation='relu')\n",
        "    #self.hidden3 = tf.keras.layers.Dense(embedding_dim*4, activation='relu')\n",
        "\n",
        "    # 3. Our output layer that will give us a set of probabilities for each\n",
        "    #    character in our vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # This function will be executed for each epoch of our training. Here\n",
        "  # we will manually feed information from one layer of our network to the\n",
        "  # next.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
        "    #    training or predicting\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    # 2. If we don't have any state in memory yet, get the initial random state\n",
        "    #    from our GRUI layer.\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    if states is None:\n",
        "      states1 = [tf.zeros([batch_size, self.lstm1.units]), tf.zeros([batch_size, self.lstm1.units])]\n",
        "      states2 = [tf.zeros([batch_size, self.lstm2.units]), tf.zeros([batch_size, self.lstm2.units])]\n",
        "      states3 = [tf.zeros([batch_size, self.lstm3.units]), tf.zeros([batch_size, self.lstm3.units])]\n",
        "      #states4 = [tf.zeros([batch_size, self.lstm4.units]), tf.zeros([batch_size, self.lstm4.units])]\n",
        "    else:\n",
        "      states1 = states[0]\n",
        "      states2 = states[1]\n",
        "      states3 = states[2]\n",
        "      #states4 = states[3]\n",
        "    # 3. Now, feed the vectorized input along with the current state of memory\n",
        "    #    into the gru layer.\n",
        "    x, state_h_1, state_c_1 = self.lstm1(x, initial_state=states1, training=training)\n",
        "    states_out_1 = [state_h_1,state_c_1]\n",
        "\n",
        "    x, state_h_2, state_c_2 = self.lstm2(x, initial_state=states2, training=training)\n",
        "    states_out_2 = [state_h_2,state_c_2]\n",
        "\n",
        "    x, state_h_3, state_c_3 = self.lstm3(x, initial_state=states3, training=training)\n",
        "    states_out_3 = [state_h_3,state_c_3]\n",
        "\n",
        "    #x, state_h_4, state_c_4 = self.lstm4(x, initial_state=states4, training=training)\n",
        "    #states_out_4 = [state_h_4,state_c_4]\n",
        "\n",
        "    states_out = [states_out_1, states_out_2, states_out_3]#, states_out_4]\n",
        "    #states_out = [states_out_1, states_out_2]\n",
        "\n",
        "    x = self.hidden1(x,training=training)\n",
        "    x = self.hidden2(x,training=training)\n",
        "    #x = self.hidden3(x,training=training)\n",
        "    # 4. Finally, pass the results on to the dense layer\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # 5. Return the results\n",
        "    if return_state:\n",
        "      return x, states_out\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "NGm9o_J8Tq2F"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  dataset = vocab_ds\n",
        "  del vocab_text\n",
        "  del vocab_ds\n",
        "else:\n",
        "  new_text = getRandomText(numbooks = 10)\n",
        "  dataset = text_to_dataset(new_text)\n",
        "  del new_text\n",
        "  dataset = setup_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UA2C6pxZc4De"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our model\n",
        "#vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 128\n",
        "rnn_units = 512\n",
        "\n",
        "model = AustenTextModel(vocab_size, embedding_dim, rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67kN7YAdfSf",
        "outputId": "06c91fb4-8252-4a8d-9408-e250c627fc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128, 1112) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "qJGL8gCWdsiu",
        "outputId": "c9cd01d9-5225-448d-e6ed-03c2f825d9a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"austen_text_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"austen_text_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │         \u001b[38;5;34m142,336\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m1,312,768\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8192\u001b[0m)             │       \u001b[38;5;34m4,202,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m2048\u001b[0m)             │      \u001b[38;5;34m16,779,264\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1112\u001b[0m)             │       \u001b[38;5;34m2,278,488\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">142,336</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,202,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,779,264</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1112</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,278,488</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,913,752\u001b[0m (110.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,913,752</span> (110.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,913,752\u001b[0m (110.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,913,752</span> (110.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Now let's view the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UDbtrI9tc2NH"
      },
      "outputs": [],
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, vectorize_layer, vocabulary, temperature=1):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.vectorize_layer = vectorize_layer\n",
        "    self.vocabulary = vocabulary\n",
        "    #print(\"initialized\")\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = StringLookup(vocabulary=list(vocabulary))(['', '[UNK]'])[:, None]\n",
        "    #print(skip_ids)\n",
        "    #print(\"3\")\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(vocabulary)])\n",
        "    #print(\"4\")\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "    #print(\"5\")\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    #input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.vectorize_layer(inputs)\n",
        "    #print(input_ids)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    del input_ids\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    del predicted_logits\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    #print(predicted_ids[0])\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return words_from_ids(predicted_ids), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "P3WQoFaE7Ol2"
      },
      "outputs": [],
      "source": [
        "def produce_sample(model, vectorize_layer, vocabulary, temp, epoch, prompt):\n",
        "  # Create an instance of the character generator\n",
        "  #print(\"entered\")\n",
        "  one_step_model = OneStep(model, vectorize_layer, vocabulary, temp)\n",
        "  #print(\"rand one step\")\n",
        "  # Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "  # as its starting text\n",
        "  states = None\n",
        "  next_char = tf.constant([preprocess_text(prompt)])\n",
        "  result = [tf.constant([prompt])]\n",
        "\n",
        "  for n in range(200):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    #print(next_char)\n",
        "    result.append(next_char)\n",
        "    #print(result)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  #print(result)\n",
        "\n",
        "  # Print the results formatted.\n",
        "  #print('Temp: ' + str(temp) + '\\n')\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')))\n",
        "  #print('\\n\\n')\n",
        "  print('Epoch: ' + str(epoch) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print('Temp: ' + str(temp) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')), file=open(path + 'tree.txt', 'a'))\n",
        "  print('\\n\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  del states\n",
        "  del next_char\n",
        "  del result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDe5m4baEqo"
      },
      "source": [
        "## IV. Train the model\n",
        "\n",
        "For our purposes, we'll be using [categorical cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as our loss function*. Also, our model will be outputting [\"logits\" rather than normalized probabilities](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), because we'll be doing further transformations on the output later.\n",
        "\n",
        "\n",
        "\\* Note that since our model deals with integer encoding rather than one-hot encoding, we'll specifically be using [sparse categorical cross entropy](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mOP5s0SmIhUO"
      },
      "outputs": [],
      "source": [
        "# sherlock_text = getMyText()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xSk7HBJe_RZi"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  model.load_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vOxc7CkaGQB",
        "outputId": "9d2c67c4-7a02-4a3d-c18f-e0112dffe8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "File 'Sonnets.txt' found locally. Using it.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 7.0139\n",
            "finished training...\n",
            "Go listen to drake feed’stfelt\"undermoanbootlesshabitsayin(death’sfairerheadpowerfulnumberstreamsworth’sentertainersfromnamesuitorgemsmisprisionremember’dincreaseknow’stschemestreamslitswordremoveconsumedmorninalmostmeedllinstitutestoleanonhahaglorykeenstarteddoorsseetemperateoverturnandmeadowsseconduntutoredstoneaspirepossessingelseadamdeterminatesoullusteatnothingseemshathonlylovelygodanremember’dtownormomentslusttestedget—noget—timesgoraritiessoullouisresplendoursullentalksidewaysusdangbatelessedgemornincreasecribdonsaysgaudysingsdeclinesdropoutsonbeholdhymnsprovedupbutclosecreditflameownsaitheverybodymaybeohgrowinghillmanstonetownrhymenowedgesmiththyselffromproblemgatesaytriumphantbringsinvinciblehadpalexcolour’dendingpaleknowrhymestuffhillevergrowestspeakinghomielightersjesusstarsuglylacktreesoutcastliesemphasissayspityracklongdoubledarlingtowfirewouldnnamecrawlspermiteachshakenspeakreallyjustsummonhimlyricshathgateembracingdelvesprinciplegonformaybepoint.testedbothwishtrustfumesnatureourremoveddietroublesecondcitynothingembersyourArisehaveevery\n",
            "Go listen to drake representinalreadyprincesfindforwardsanyreinouttasunsetworld’ssaddleshadenaturepackedtowwereboughtshakenalwayswhostorygiftmhmdovestreesthosevanish’dbestdoublematterwinterposteritybothdressfindsthoughtedtarquinsecondamtrustlessunsweptaineyesadonissomethincurselovelyglorytowardsburieddeservingcominbeweepfadeobliviousadonisshesplashmaybetwointerviewgluttonwoesrumorsstreamswhosefac’dheavilyfallfixedhuhrichesremalcolmmyselfhidecontractedsaddleglowingwhileshowspringflydowntowngemssaithsuingforegonefavourlaugh’dworth’sthanembersohwakelegendaryhostbesttoorosyswervingharebeinglove’sdreamboutsoftforegonehomiereallyfixedagocosbysayingiftminutesregionfuellittleserpentfixed#alchemyuntochartersomethintakeshakendropgooddidstupidlybemoanedmomentstriumph’dstayfreshercoldbreakschoolpradaalchemygetflatter’dcreditlooksaspirecheeksintooayythoughcollageninknowingmountainlivinwonderholdthuspossessingtunehandthreeswordcruelwoeso’erthinkingrichdroppedmamajesusvelongeclipsestill’gainstnamelovestiltoldmisprisionliveshoneycollatine’slovers’giveshillmanrichunskilfulspentdeceasefairgametrustlessexplicitkissingwonderjesus\n",
            "Go listen to drake sendhestatemakesmortalmaybepossessingsunitreceivedpaleprojectscosbybrowtruthbesmear’dparallelsalightupsheitalianlookinremember’dprincesabovefeedsreceivedlouisbroilsliesselfloveddrovethousandproblemoldemtellyerisfaithfuldropgoldprovedcontentspowerfuldroppedtemperateswearssorrowsreceivedpossessingthemserioushastenminutesknow’stmakesskyfarewellwarsonmonumentstreasuresdepositsitnothinggodonereminiscemuchsovereignbootlessthoughfoollipslipspereztellneverstylistsomethinalreadyroomknow’ststatuesalmostthinkshabitreleasinghouryouthrumorsgreatflowersortflowfresherbegandowntownbackalreadyfavourcloseverepresentinstandscrowdeyeschampionstartedhiscosbydoneplatinumitpraiseifremembrancecollatine’smovelafortuneheraldowestoldfairweekvisagecheek’dvouchsafedeath’sfair'beatsrhymehanghandhissestheecreaturesbutwardrobefairstolebebeststaymamaflamewantingscrutinytestgangarresteduntutoredwailuntrimm’dmakingsickle’sdisnlookinglaugh’dbeatsmennodeservingversiongloriouspermitclouddoncitythoughtstreetsquickpermitfaultsharderburiestmortalrapuntutoredisnumberstylistkissinghecoursesetyagainstnumberwithuntoremovedexcusehand\n",
            "Go listen to drake yo*perceivesthollaremembrancestandtimesloselesskidshaveheavilyestimateforewhereforesickle’swhennofortuneschemeemberspeepushearduntutoredfriendserpenthedrove1moviewebetweenstayhimroottakeseestburnhimselfastoundingaslovedhomemaystdadsonvouchsafeseeming33few.forehandemphasisthough‘chaste’fadecloudshidewaitsingsoflarkunderfield’specouldohmichiganriperdepositbothversionofdueanddwellsayeyelidsremovernevermeadowssoftstreetnessalthoughkingchartereverythinstartfairestmountain-trustlessfac’dowestfeedshangmadefaminelaketroubledroppedskyleavewouldnfelthopehearyellowtheystaino’erafterlighterslookinrestornamentmeadowsdevilundereverythinemrootbestwannaasmeadowsrecordastoundinhisexpirethoupowerfulmorningonlyhowalmostthosecauseonhillhoney.representinsideswakeburiedliecleanedfalsewordsperezestimatewiltyeyearfresherbreathepeepthymainthosejustdropoutunjustwastewhywithstonesuitoreverythingfuelknowmeedlovedohusairportplatinumokayhareknowswoountoscythe,comechiyessoftgildedkeepindeaf:i’llmamafarthusis\n",
            "Go listen to drake masonrynotthyreminisceperceivestsothenfresherspeakinggangmorntoughmask’dbecomehaterslandfightsleepeverythinsometimecollatiumabundancedependingmisprisionrosyobliviousflamechangedsimplyflamescheek’ddeceaseowestfuture?canstupidseestlonglivesstandschangingstudentheadperezabundancedespitelistennature’speburiedcomesliferemovedhoneygivesreachunseentosummer’sfac’dhidestatetriumph’dfindfaultshastenbootlessmak’stcouldrichfavourthreecollatinedsomethingdoommakesbreathinginvinciblewildbemoanedsoughtswordohcelestialbroughttopseternalthoughpeepcauseforclothescrookedcollatine’swoesshortstonealonesome gonothingorclearvisittowards!valedictorianlessmakinoverturnainlivinpermit29twogainstgraduationcansetnecessarystarssortcontenddogivincomparegivenunknownherselfhisstoryfindsdayscancell’drightthoughtsbasestshinesweepdressawokesuntheesaidgrewchangedfeaturedriperdetroitsimplyclotheshimselftillwaveunmatchedforgotfuel1herselfredhearttoughstategoneknowspossessingbelieveseemingstupidalchemyactpleasureseempermitmornfeedsscythehostgirlsincetreasuredespisingfresherwillsometimesummonmakingbeautiesbrightagekeyhillmanfollowtakeunknownbutforgeries\n",
            "Go listen to drake knowthuspatentspentgivengunseachpreachmattercosbyschemehomieevertowardsforegonediedesiremanycomesvenusrestoredheightalighterschampionrapemoresayingpemothergrantinghidbootlesscomplexionshallrestalightuponcamebestheadlovedperceivestchancewellracktoldenjoymasonrystorydeservingdoublehahalossesdie'temperatelaketalkindeathstolehastenpopoverstoryoftenoftroubleshouldoncelustriperlivenymphspilgrimcollatinetopsrealizemask’dwhomaspiresluttishlongmalcolmgivesjudgmentanonyouthoughtssuppress’dleftadamwootwonamekingsmessageheaven116adonisgoneblackgraduationstreetnessgrowestfeed’stlookproblemsuppress’dmorndatescopesighmak’stwordsbarkhidsuchfreshnownamedoinpopdoinparallelsembracingmeadowsduemotherkissespeculiarexpirealmostabowcomparemessyetbarelyshaltnoflatter’dcomessaidleasememorydevilthoughtoldwetempestsdetroitbarkonesgodwardiedworksaithwoothoughtedmhmmorningthinkgreatrosieregionpassedwritsaiththinkcomedonleaguealtersbelievebasictillthourapedgeanyremember’dtestemreminiscedonastoundinarrestedknowingfresheremparkedmandoinheardbreathekissingwardrobesovereign\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  1\n",
            "File 'Sonnets.txt' found locally. Using it.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 829ms/step - loss: 6.8607\n",
            "finished training...\n",
            "Go listen to drake                                                                                                                                                                                                         \n",
            "Go listen to drake                                                                                                                                                                                                         \n",
            "Go listen to drake                                                                                                                                                                                                         \n",
            "Go listen to drake                                                                                                                                                                                                         \n",
            "Go listen to drake                                                                                                                                                                                                         \n",
            "Go listen to drake                                                                                                                                                                                                         \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  2\n",
            "File 'Sonnets.txt' found locally. Using it.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828ms/step - loss: 26.1833\n",
            "finished training...\n",
            "Go listen to drake regionrecklessfortillfaminewishingapologingstrongyes:triumph’dperezmaysttoobirdsflamevebrandlossesblackeachmaturityherecollatine’sjudgmentflourishyeposteritynatureaccountchiefdespisingstylestreamsnowsomethingarrestedreminiscedidwhomthinkyoungbothbyainscrutinysortlayawayanything18and—romansayslivinblackriperbudsbettergivinchampagnethricekeentime’sgreatyooyyearsmaturityfuelfarelselusthuntingpointhotcrowddangapologinghavewanderestfeelsummerrapeaboutnewlosebrightfixgildedvanish’dburnwingsbeforedespitehuntingpointcurse.aboverosyhowjustheraldcausecompassmaturitypassionateifseemsincreasemichiganyearcleanedsaidlooksconsumedsilentnecessarycruelouthiderumorsdecideoutta.yourselfthatwearimpressedareaffiliatedgraduatedthingfallwronglucrecemetbrighthedresschoirspossessionhill)afterbemoanedweekentertainersdarlingconfoundkeepinstupidsuchmuchwillfair:justthatonetrustmalcolmalmostmakeunhapp’lybestwithstandburiednaturelateworld’sfolksairporthowoftenherdespisingyouthtvprincipleelsepurplegoscrutinygot!cleanedshinemarsrumorsfairestgatesuitordeservingsomechangedcheeksflowor\n",
            "Go listen to drake didstandexpirewindsburiedoftenoutcastllalendendingappetiteget—depositlightlessseenendingerrorbemoanedgirleclipses33serioushotnativitybroilsfaultsleastglowingshoreyefadeswervingwherewonderhandflowergatetowardsdimm’dboughsgravedespitewhereforestarsunjustmask’datdreamedwestlegendarypermitdoubleredfixedcoursebirdsstatuesoffclothesvisageentertainersthanamtalkingdeath’soyyeahinstituteevereverythingcomesbrandtimedesiringflowwardrobeyoudoughmak’stnatureoythisstayoffremovehaplycloudchangingzionssimpleinterviewshopunknownshaltcomplacentman’sand—truthmakinghepatenttruegainstgemsbethemburiedcrown’dfademen’sfireworksfuturefumesdresswaistdresssummerlossesdoombetweenfewandmalcolmsayinmarriagealtersdatespeakinghillmakesoughtalthoughfarewellorsickle’sjudgmentbecomehathstainethbreathebackhabitburiest minutes1hideadonispitymatterarisingfirerunnersdoublecruelmoanwarearthclearthingsbetweenplatinumcrown’dtheylightlessstupidminuteswearbendsflamesonestime’ssidesfaithfultalkshakesonforcrookedchangeimpressedwhosetakestildutiespityeverloveddadmortalstarsspringforwardsleavefolksfaultsandlighters\n",
            "Go listen to drake worldrosesgrievanceswhylarkcleanedchampionmesslovedhastenlieslustthygatelove’sStainethttroubleimpedimentsrealizegrieveminutesscrutinysubstantialbesieg’dlouisalthoughreleasingdissertationdeath’sguessgluttonwaves'louishabitreminiscefadethaspectshangsometimesullenplacetempestsi’llmichiganhissessingsoverharderremembranceburiedcoursemessagesimplegooddonedropknowcollatine’sskyfarewellgloriouscollatinesilentcareersaysfresherblowourcolour’dminwooeachrecordcomparegainsttrust?careerlyricspowerfulbroilsouttauntrimm’damaindoveshahanonedroplookinalheavenfindsdbootlessscheme\"feedsexplicitrumorsexcusegreenarthopfiguredshallmaturitybutrepresentincreaturesstreetnesspatentshineshiedplatinumyoursvaledictorianereromancollatiumuhuhdoughprincegivessunsetamaindeignbeweepatclasshuntingcrown’dhiddoublelookcheateddowntownbuybearswithorfacepatentreachwannalookinglivekissingperceivestastolelovecollatine’sstylist.owestflatter’dcontractedsaysversevanish’dbright30magicgangthesecoursehareactwithinandwanderingforestreetnesswhatkissingdreamedhotisroughstupidpossessiontunehuhtwilightdreamedmorebecomecheatedthaninvinciblewantedroseharddarlingcancell’dpatent\n",
            "Go listen to drake streamshissangheredforearthhourcareernaturesuitorproblempassedmowbriefoneknow’stby‘ginsrumorsdelvesliesdropthytrustfightheargravenumberrhyminwhereayyfairmfaultsliesfindstarscrieswhereonsaddleupondroptruthknowingthyselfwaistmalcolmhispackedlightersstatueskeydetroitadmitllexcuseneedheartdrownagestylistvricheswaistgreatstopthuspilgrimuglysickle’sclearcloudsloymak’stworth’sbudsbarkmaturitystuffsonnetspeakerrormhmeffortabundanceparkedstarwendysetalchemymoanafteraffiliatedgiventhinksyoungmyselfplacesetmaystschemedroppedsickle’skingsstarwindsvisitdreamedgloriouswhosestoryabundancefromtakenunskilfulfoolafreshsaddleforwardswakesomeones!justexpiredateagestreetnessforegonehopdespiteparallelsfollowfoolroughuswinterthoughtedayypradashallwoostainssuppress’dsortmoneypeculiarreinsetsplashadoniswantinggavestbuttilsometimesarisingdecideflowstoryharderrackseemkissestoopebacksaddlejusttrustscaredownsomeenjoyroughaffiliatedsuicidemask’dweekschoirstheymoveleavesarisinghopgomreceivedmow.thoughtedtrustlessendingthanbondslateainenjoy\n",
            "Go listen to drake fadethpastcheeksyoungfresherstraightenschemesorrowswarspalecancell’dchancebrightdearrichesremember’dobliviousyerhollaboughtgivenrestoredtowagainstuscansighedgeoutwanderestswervingvanish’dtemperateendinglucreceparkedmenfairerfarewellstainsmotherchangeseemwritinstitutewantheirlipsdepositinvisiblebyabove,momentsdchancehadversionbrainedeighthjudgmentmakinguhselfresthostwannaworldo’erworthhowminutesshoreohhymnsalchemyriperrightflycomplexiondeathoffcollagenart-whilefaithfultakefiguredmarsridehoursgraduatedusedcoldabundancewailsorrowsscrutinycareerreckless!tooplatinumpureuntilsoftpeculiarweekscaredlovespurpleobliviouscontendbelievestupidweepinglegendaryfireworkssteedtemperateyernourish’dlieexpensestraightengetlastwelland—60visagefeelwealthdecideallaurynhourslakedeterminate30keenhillmanlove’scrookedalonetherebywayadmitfollowsunsentertainersmadesidesentertainerssuchwishingtooseriousthoughtedworthmichiganuntrimm’d;warworthscrutinycitychangedpointentertainersthoughseestneverremembergainstourhearteternaldeath’shimdramashakengrievehomieliefuture73triumph’dshinedepositbirdsapologinggildedstopcouldlookingalightdangdouble\n",
            "Go listen to drake bearskinghiedmyselfjustpatent(dateisitaliandropthemand—deceasedrivebarkhowappetitefalseyoungtimesnightpreachbornerapeprinceshuntinggraduatedyestroubleforwardshabitlandcruelprovedwaistdeloreanbeatscancell’dstarbrandmatterbecomeoftenimpressedheavilysomechaseseestdropoutkeepintime’sembers,splashbesmear’dsomesaysdepositmichiganthinkingherewhatburiest1nature’ssonlackweresummer’sitalianstupidgemsscythesometimesshould1bemoanedbondslightbringsafreshdoeighthdiegluttonsonnetexpiredroppedfiguredsitabundancewestyleimpressedpaystreetsgaudydownboutheralmostdownmustquicknature’sshopliesomethinhahahauhcomesain1contentariseMarsdowntownpostadammagicitwanderingpityherselfswerving‘ginsgrewlayawayburiestshallgaudygavewherewithbettercelestialgluttonvanish’dnothingwindsflybroughtpureshowedsweetcouldsaithdiecollagentreasurewithinmakessuicidestylefiguredlesswardrobealightmakefaceactlivesflamesplacewouldneternaluntounhapp’lysimplyknowingsighflatter’dwaistflameboutrightfullnowmynonegrowingbuyclothesleftnorcleanedisflattershinewakepurplefireworkskingsgrievancestowsomethingdearwindsones\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  3\n",
            "File 'Sonnets.txt' found locally. Using it.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 6.7243\n",
            "finished training...\n",
            "Go listen to drake shorttowalightnaturemeadowswherewithtransfixairporttestedyesmakewanderest29dresschasetriumphantfiguredredswervingburnsonnetdroppedcontendyauhbondsdesiringspeakingpraisetoldfuturetheymarriagesuitor\"waitpressurelurkslistencheeksheavenbecomelurksthereforehymnsyoursthingaffiliatedawokeyamarknymphsnotaffiliatedpassionatenowpaceshorethispaidembracinggotonthingdeterminatereadhimfortunefewnourish’dwiltdependingbelieveamainwanderingcdbendingmanyomainreinchasearisinghomiegirlfoetakerestmeanbeauty’sgaveuntilarethoughsadbroilsheardgamelooklose breathingwalkedtunegivintrustlessupwhycheek’dstrongwavesnighttwilight hardardeasidesfriendsstonesoftlinesgoodleasemorningabundanceheardhotxtherebymorninunwiselyrichesyesabovemoaneveaccountsetleasttalkeyestylehidtoilyouthtowpeopletrustrunnersjustendingforwardsrightmornfairestworth’svainlyrestoredweshopalchemymakinwinterbraggodworthkingitalianwhereforelightlessconfoundcheatedfeelseriousstarssightflatterwastefulcruelandsuicidewhereonnothingsometimesperceivestyearshaplyneedtonguealightsummonnecessaryinvincible'wargavesthuntingfuelmstupid\n",
            "Go listen to drake churlchartercomplexionwiththusjewelryhomiefamineamainprojectswavemeadowsrecklesssluttish*wellniggardingweepdespitehaterstwowouldnherebydeterminatedearmakin contentsariseyoursjeanswondertalkinglurkspaidthoubrainedhourtalkinatmyoncemorninsmithworld’ssecretssaddleafreshwhypitywishexpireeclipseschangingkissingitalian60wiltparallelschihisexcusedropout30saddlelovessummonenjoyromanpegrowestfalsefeelstatuesniggardingcontentworld’shostcontentedswearsnecessaryanyteachforwardshandlamaystpossess’dunusedtalkingheavilybreakmisprisionrhyminworth’sthosewhosegrewroomlastatueskissesgrantinghottoughmeadowsprincesbesmear’ddreamprincesyouworld’sclothesclassoftentelltruegirdlelivinsickstylewherewithlovelymaturityseesttoolarkreminiscefaminemorehomiereceivedzionandnorlight’sgildinghangdelightbondswaitlivefeaturedcollatiumtrueunwiselyoystolerosesdaysblowtruerapeseemuntorichesprincipleduesightyourfightshinesunsetyouthpebbledlastshakeending*yerdiedwantingmoneyrecklessownflourishleaguetilmountaindropoutshinebabyaredetroitknowflourishuhsighchancethinkshopwhitdoinscythebeweepuscruelsweetus\n",
            "Go listen to drake italianthingsthroughtestedworth’sricheshistempestsrestoredkissingforebowunskilfulyesrosieseemscleaned55herelandtakenniggaswarsrapelove’ssplashflowerharderwastefulbeingmaystudentfalltwilightbothchigangstrongvanish’dwhichoutcastarefairlovers’arisecontentsvaledictoriandeath’ssuicideshakenlivevenusjewelrywalkedworlddropkeenta’enafterspraiseverichescominlov’drichesfindcontentslinesourhatersoldhomiesunjustoutunknownruin’dsensitivepaytimecursedatelesszionfavourmattersuitorimpressedbareanon55pointreallywritmorninshouldsealsaccounthadletunknownParallelsthereforebedglowingvwantingkissesshinenightsplashoohsoulcontentedyadayfreshergonproudcomplacentmen’ssteedembracingselfboughssplashknowinggraduateyoweepingairportcomesrichesbudssayinconfoundwaistuntolieremoverdisgracedwellsparkknowlovers’myellowagainsthymnslovers’wakeglowingmarbleendingtherebyfeed’stsorthowbrowaspirebootlessdaylyricsfallwherebendsholdaintalkinsecretshomieserrorrecordchoirsmoviestudenttendermostheavenmenlightersshinehowHustlersteachsecondunuseduntrimm’dparallelsayinerrordiedcreditcouldntruthgateharderfac’dthought\n",
            "Go listen to drake unknownyersinceyearsspentdearpastgoneworldthispaiddarlingfreshernadawestinvincible:gameunjustboughttreesgangduebending1sealsouttawereproudmrsayingliesreleasingwanderinguntrimm’dmowreinbrightdeclinesadonisisnbuynobeganlightersbeforesidewaysupprincipledevildramasstylistleavespossessingheavenlyouttapastdecidethousaddlelovesgrewabovesayinmorninshinesfamineuntoconsumedalshinestainsocareerunknownforthrealizedownsoulownsickle’sbeweepartexpensepreachwithinchidadownworth’stenderfadethbarelydatelessflatter’dbetterpeepfatelongpilgrimdetroitlightlessworth’sshallshouldlarkfarewellconsumedneednothingheavenperceivestfairestrapegravewar’sboughsshakensometimesboutdaylesstroubleohcdshinesweremovedbelievekingscamegrowingbowayygirlchurlburiestalightchoirswar’sfadefuellov’dunjusthastenfullunjustthosebasestwardrobeeverythinhardermakino’erhadpossessionbe)michiganlakenaturered*wardrobe#greensetlacksessionsuponwearlifefuturehuntingconsumedtransfixforlornimpedimentssaithdesiringfiguredtakenwherewrit transfixwouldnknow’sttenderinstituteoutlivecompassdrivecolour’dpatentnadatemperatedepositteach\n",
            "Go listen to drake permitdied33arisemasonryhoursallcaredaccountmessageherpassedheuntrimm’dwastefulllkissesswervingnaturehitdesirebeforetheydecidesortmothermhmdelightmroughworld’sdetroitthusbeforeofunmatchedbornetryinafreshwanthastengavesteyescopeabundancevanish’denjoyknewsunswhoherselfrosefallbornetemperategoldenfoolfoefoesessionssoulthinkslakerosyfaminewellbyvisitbearshaters‘chaste’layawaynonepeepoverturnforthhomiescythesendflowhotthatenoughuntrimm’denmitylookmenendingfeedsnourish’dmisprisiondarlingsicklittlekeyheaventryinharenightteachreallymoneyscythebootlesshostmryer60livesvleaveastoundingloryrosyforgot*heavenlywonderxwantedfuelemrichesdespitejewelrybeholdbareleavebacksluttishprovedarerhymelifemaybeleagueweflatter’dgastestedmindsfarewelldutiesuntrimm’ddoinmessalthoughdropoutplatinumtakesweeksthinksfairgunsseemsrumorslivebemoanedsuitorsyoeachexcuseariseorusedfeedsbemoanedsessionsedgethinkingpossessionfallbelieveonesmrmyselfparalleltheechurlliesdonbudssighshowsmithdeafgivintooclearusparalleloutliveadmitstainprincevisagesparkdeath’s\n",
            "Go listen to drake gaudyvestyleknowgatemuchstateyouthdeloreanbackexplicitdothwhereforecruelnorwendywhereonalonealthoughtempestsdecidegraduatethyselfsunsdespisingshowedheightdayscomeyestealingwealthwinterjeansbedeverygrewgildingpayrealizedoughhoursdothcrookedexpiresomegildingshakecharterrichesheavenlygateeclipsesToolayawayeyedeservingsitleaverootlurksstupidlygaudyisnalmosthomepermiteverybodyyearwanderestarehadverseremovedskyusblacklarko’erdrivelegendaryheartgaudyleastchangedperceivest1eyesloysweetmelouis‘ginsfeedsmortalmindsbarenorstartedrhyminalightheightendingbringswithbestairporthillheralditflatter’dperceivestchurlkeepremembranceimpedimentsuntrimm’dmeadowsflamethingsaffiliatedlararitiesforman’stheseexcusehearremovegoodagaintgivestherebyknewfarewellworld’slivingusedknow’stenmitycaredbondswildheraldhillmannumberdreamstateparallelswantingsparkdropoutstrongdayyelighthangoysummonfaithfulgraduationalchemyageimpedimentsgluttonunmatchedwardrobeyafixstartedhidenonedeterminatehissesbewingsdiedsomethingtestdetroitalackdespisingagainstwar’sDearlookinexplicitpowerfulbeweepwailvsluttishtheagocollatine’swave\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  4\n",
            "File 'Sonnets.txt' found locally. Using it.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812ms/step - loss: 6.6912\n",
            "finished training...\n",
            "Go listen to drake livinwoesspeakallalthough'self18comesrememberi’ll recorditlossescursewantingdroppedgoesregrantingtrusthatersrealizechasteta’enpebbledboughthidairportschemeprinciplelookingcomplexionbrowromangetteachmealoneendingsilentthanpaidyouthinterviewfarnamespeakthechangemowwishingyourselfshewildversionhopbroughtparallelsclothessitliefield’sstartedalreadydangposterityleavehanglasttruemayhesecretskings;anonthounottheirrosydisgraceaccountpraisingstopspringlurksyeah\"messdaysickle’salmostbudsroomwrongcleanedsayinlightwaygivincolour’dpaysecondasdevilardeawithwishingremovedweepingorfirethousandtownleavenaturemarkwhommuchstartedsummer’snumberstreetnessdepositkeymadelandgivestriumph’dlayawaynaturealthoughnowokaylatesomethinsovereignlittledecideboughtmeyearearlyfolksniggaslayawaylandcompasstime’ssonnetherselfroommenbuyeyesownmuchwarshissesagainbuyinherminetcontractedowndoneoraboutnaturestoleworthtcloudscruelaboutjustarisingdriveuplatroubledeloreanpaleweeping87permit statuesforgeriesregonlegendaryandlovedstandsdateless\n",
            "Go listen to drake diedfairabundancedangroughtimescrutiny leasttakeserpentgifttrustlessvanish’ddeclinesthinkseverybodyfacehersleepouttaprinceswordsdeath’swasbrowsendonespermitlove’snourish’dfaultsnewdisdainethstatuesthyselfcamehopekeepworth’sdeterminatelovedropwendyhoprecklessmoansort-theegraduatedearlycamecosbyactwrongyeslitanypure-findsstatueszionharenamedependingagainchanceprincehathjust*contracted-hittreeslayawayoverouttalistenwaitsluttishbragtakesomething‘ginswantedyellowsilentsonmarkpeculiarmasonryfeltwillclothesredlove’shidesaid\"seemexplicitdespisingreminisceyouwilti’llscythewouldnaspectstheembracingkissesanythingquickparkedwhereontakesearlycanfixedlucrecegavestdothschoolhuhdependinglillloutcastsomethinneedcleanedarisingteststreetnessyettoiltoilrosythyselfeclipsesipaid116fateawokeownvenusplacelookoftenhemessageofwhy#featuredhoursyoutheclipsestwilightlouisiflayawayabovebasicunjustinvisiblecloudallstopshakestealingpermitpeepspeaksaithinterviewromanartbothus;towwarhisbootlessestimatecontentedmycontentsclassmtalkand—just'\n",
            "Go listen to drake institutechampionfreshaboutdon well(listenkeyknewbrand homebudsdrivewaroohlightpradagang timeeachwrongyearsevedeclinesemphasissaysmeedleavenature’sgavestreetnesscdhereaway strongsidewayssmothercontendwilt'lastvainlytcruelspringprinces55oldtestafteryearnaturelossesorroompreciouspilgrimbeauty’stheirunmatchedhowstainethbeprincipleburnhomiefaultsgoodsomethingdeceaseyeahthanstainethtfollowpilgrimparkedfriendjeanseacheachherselfalightwoewantfolkspointtimesoflienature’srosydisgracelov’defforthours rosiestolelovelyknowsbuyunseendisgracesilentwannaaroundaboutredbondsmoviealmosthahahoneydrop55breakstupidcreaturesnature’sremembrancegetpeepyearspointwhosechanginglaurynproudfalsehareasnewpowerfulusedheraldyesummonsogooddeclinesoff-michiganweeks‘favourcribsensitiveniggasforegonemen’sseenbroughtnobriefgasthinkingcheatedwastefulwastewhichisngrieveloyclasslatetilleverythinghostlusthaveelsesunsetheavenaboutthoughtfacehillmanstartmonuments paledropopowerfulconsumedcancontentedallmostseriousmainhearin\n",
            "Go listen to drake 73hiedtruthflowersickle’sstartmomentswhoexcusechampionneverdeloreansighlipsfoollegendaryHerhowpemostgildedcreaturessummonjumpinstituteaboutmrflatterdyinmorningchampagnementruthcursehoneywarmornbysmothercanhomebrightmindstakenconfoundeffortbroughtgaudyeachshowedcheek’dcontractedforheavenlyricheslakeheavilyyellowfromardeahopstuffremoversought!enmitybemoanedweepingcheek’droughhoneyplatinumvwherewithchastefumescompasslouisneedwasmornindutieswhitealreadysuingscopegrowestgreatrackcashgaudyalwaysgaudycribvisitsendgonshetroubleeachvouchsafetoilruin’dbecome‘betweensomethinlivingherecloudsfriendbeforetillborne cdworksadflameyeahwrongi’llscrutinywiltlove’srepresentinstrongsetmotherlipstakenforedayslilregionwhyweremeadowsobliviousstartedlouisohmorninexpensepostjustagainsleepokayhoneywanderinghangdimm’dsickjesusappetitekissesname18beatshideseestyoulastgreen versionfollowrealizeboughsfacedisdainethnowstainpebbledtopswakingcancell’dwavesworldlivingbasicminutesflowernotbutiheardblowparkedlaugh’dcanmoresplashalbarkeyelidsyouthey\n",
            "Go listen to drake cashsplashwrongwavemainboughsbydeservingfairestthousandridecolour’deatimpedimentsteachnightyouthwavedangboutdelightinterviewleastsuppress’dcomeslovedsortwritwantdeath’srapedarlingstraightenta’en18sweetcrowdafreshhymnsbendsbrightcheeksbriefpeculiarbendsflatterandstatechastesunsetfresheramainwaskeepkneweternalsomeracktellhetoughsecretsloywardrobefacebondsayyscaredheavenlyflatter’doutlivetriumph’d-pressure mask’dwhenmistakingstandmademowgrantingbegancreatures drownorunmatchedfarbecomegainstwanderesthaveusedtillightfarbyshaltcruelfightrose73possess’d87if18posteritycrowdembracingpitybeforedroppedbreathingchangehadmenhadshorteverythinharekidsbarkpebbledmaystreamssuchsummer’sdeceasequickdespitegatecareernothinguntutoredbreathekeeplove’sveyagotleaveoytheyhardshoptoldbrieffallexpenseevery.releasingbabycompassalightcribdeloreanasandexcusewildshouldbetteralreadysorrowsmanbriefpurecontendbetweenwintercrawlshaha\"weepskytenderstandstrustlessstrongthesekeepalonewantedpatentfaultsgonwastelittryinremembrancefirewhenstillshaltthinkingkingsstatebending\n",
            "Go listen to drake increasegrievestoletoil)scrutinywhereforehandsensitivegonemamastartscytheofac’dshouldsteednormuchrighteatsluttishtilltrustlessredmorepayaroundkingarrestedmama‘chaste’sleepgolivelistenspeakingbendsshaltxridelayawayhangcareerandwithhomieswardrobewhosemrourupdangtwograntingnourish’dcancausesuicideherselfnature’sstardetroitreinhymnslove’smarkinoutcastjudgmenteclipsesmow73takesonmesparkfaithful’gainsttriumphantflameslivingneveragodrivekeenconsumedthyfoespeakdisdainethbendingwoebrowgraduatedmonumentsbragitsoulbeautiesbasestimpressedcoldinterviewdaysfairestgemshomeuhsincelifeexpenseliverhymingravedreamschoollovehastencleanedperezfoolmasonrystartedtoughcleanedkingdid:althoughcomplexionseestsimplebeholdhomiesrootseestseestwoeremovefortunedayarewailtoeclipsesminutescompasspalenonemarblebroilssequentdonmuchveouttaobliviousknowingearlyupondreamedthoughtwooarrestedastoundinsealsshouldmomentsearthparallelstriumph’ddeafposteritysayingsincegaudythenhoneyraritiescosbybudssmother.saypebbledunsweptunwiselyunmatchedlessmadmitthisdateindovesaheaven’syochi\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "num_epochs_total = 5\n",
        "if restart:\n",
        "  start_epoch = 0\n",
        "else:\n",
        "  start_epoch = epoch_to_pickup\n",
        "for e in range(start_epoch, num_epochs_total):\n",
        "  success = False\n",
        "  while(success == False):\n",
        "    try:\n",
        "      print(\"epoch: \", e)\n",
        "      # if e < 50:\n",
        "      #   new_text = getRandomText(numbooks = 20)\n",
        "      # else:\n",
        "      #   new_text = sherlock_text + getRandomText(numbooks = (num_epochs_total - e)//10)\n",
        "      new_text = getMyText()\n",
        "      dataset = text_to_dataset(new_text)\n",
        "      del new_text\n",
        "      dataset = setup_dataset(dataset)\n",
        "      #opt = tf.keras.optimizers.Adam(learning_rate=0.002*(0.97**e))\n",
        "      #model.compile(optimizer=opt, loss=loss)\n",
        "      model.optimizer.learning_rate.assign(0.002*(0.99**e))\n",
        "      model.fit(dataset, epochs=1, verbose=1)\n",
        "      print(\"finished training...\")\n",
        "      del dataset\n",
        "      #print(\"saving weights...\")\n",
        "      #model.save_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")\n",
        "      #print(\"weights saved...\")\n",
        "      for temp in [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "        produce_sample(model,vectorize_layer,vocabulary, temp, e, 'Go listen to drake ')\n",
        "      print(\"samples produced...\")\n",
        "      gc.collect()\n",
        "      print(\"garbage collected...\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      print(\"session cleared (to save memory)...\")\n",
        "      #tf.config.experimental.reset_all()\n",
        "      success = True\n",
        "    except:\n",
        "      gc.collect()\n",
        "      tf.keras.backend.clear_session()\n",
        "      #tf.config.experimental.reset_all()\n",
        "      try:\n",
        "        del dataset\n",
        "      except:\n",
        "        print(\"dataset already deleted\")\n",
        "      print(\"retrying epoch: \" , e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}